{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning & PyTorch Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Numpy Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "t = np.array([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of t : 1\n",
      "shape of t : (7,)\n"
     ]
    }
   ],
   "source": [
    "print('Rank of t :', t.ndim)\n",
    "print('shape of t :', t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t[0] t[1] t[-1] = 0.0 1.0 6.0\n",
      "t[2:5] t[4:-1]  = [2. 3. 4.] [4. 5.]\n",
      "t[:2] t[3:]     = [0. 1.] [3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "print('t[0] t[1] t[-1] =', t[0], t[1], t[-1])\n",
    "print('t[2:5] t[4:-1]  =', t[2:5], t[4:-1])\n",
    "print('t[:2] t[3:]     =', t[:2], t[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]\n",
      " [10. 11. 12.]]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[1., 2., 3.],\n",
    "              [4., 5., 6.],\n",
    "              [7., 8., 9.],\n",
    "              [10., 11., 12.]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of t : 2\n",
      "shape of t : (4, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Rank of t :', t.ndim)\n",
    "print('shape of t :', t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "tensor(0.) tensor(1.) tensor(6.)\n",
      "tensor([2., 3., 4.]) tensor([4., 5.])\n",
      "tensor([0., 1.]) tensor([3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim())\n",
    "print(t.shape)\n",
    "print(t.size())\n",
    "print(t[0], t[1], t[-1])\n",
    "print(t[2:5], t[4:-1])\n",
    "print(t[:2], t[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([4, 3])\n",
      "tensor([ 3.,  6.,  9., 12.])\n",
      "torch.Size([4])\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 4.,  5.],\n",
      "        [ 7.,  8.],\n",
      "        [10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim())\n",
    "print(t.size())\n",
    "print(t[:,-1])\n",
    "print(t[:,1].size())\n",
    "print(t[:, :-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5.])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([3,3])\n",
    "m2 = torch.FloatTensor([2,2])\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 5.])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([1,2])\n",
    "m2 = torch.FloatTensor([3])\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([1,2])\n",
    "m2 = torch.FloatTensor([[3],[4]])\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication vs Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "Mul vs Matmul\n",
      "====================\n",
      "shape of Matrix 1 :  torch.Size([2, 2])\n",
      "shape of Matrix 2 :  torch.Size([2, 1])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n",
      "shape of Matrix 1 :  torch.Size([2, 2])\n",
      "shape of Matrix 2 :  torch.Size([2, 1])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('='*20)\n",
    "print('Mul vs Matmul')\n",
    "print('='*20)\n",
    "\n",
    "m1 = torch.FloatTensor([[1,2],[3,4]])\n",
    "m2 = torch.FloatTensor([[1],[2]])\n",
    "print('shape of Matrix 1 : ', m1.shape)\n",
    "print('shape of Matrix 2 : ', m2.shape)\n",
    "print(m1.matmul(m2))\n",
    "\n",
    "m1 = torch.FloatTensor([[1,2],[3,4]])\n",
    "m2 = torch.FloatTensor([[1],[2]])\n",
    "print('shape of Matrix 1 : ', m1.shape)\n",
    "print('shape of Matrix 2 : ', m2.shape)\n",
    "print(m1 * m2)\n",
    "print(m1.mul(m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5000)\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([1., 2.])\n",
    "print(t.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can only calculate the mean of floating types. Got Long instead.\n"
     ]
    }
   ],
   "source": [
    "t = torch.LongTensor([1, 2])\n",
    "\n",
    "try : \n",
    "    print(t.mean())\n",
    "except Exception as exc : \n",
    "    print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1,2], [3,4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000)\n",
      "tensor([2., 3.])\n",
      "tensor([1.5000, 3.5000])\n",
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "print(t.mean())\n",
    "print(t.mean(dim=0))\n",
    "print(t.mean(dim=1))\n",
    "print(t.mean(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1,2], [3,4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor([4., 6.])\n",
      "tensor([3., 7.])\n",
      "tensor([3., 7.])\n"
     ]
    }
   ],
   "source": [
    "print(t.sum())\n",
    "print(t.sum(dim=0))\n",
    "print(t.sum(dim=1))\n",
    "print(t.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max and Argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1,2], [3,4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "print(t.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "Max : tensor([3., 4.])\n",
      "Argmax : tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim=0))\n",
    "print('Max :', t.max(dim=0)[0])\n",
    "print('Argmax :', t.max(dim=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim=1))\n",
    "print(t.max(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View(Reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[[0,1,2],\n",
    "               [3,4,5]],\n",
    "                \n",
    "              [[6,7,8],\n",
    "               [9,10,11]]])\n",
    "ft = torch.FloatTensor(t)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1,3]))\n",
    "print(ft.view([-1,3]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1,1,3]))\n",
    "print(ft.view([-1,1,3]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.squeeze())\n",
    "print(ft.squeeze().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.Tensor([0,1,2])\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(dim=0))\n",
    "print(ft.unsqueeze(dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([1,-1]))\n",
    "print(ft.view([1,-1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(1))\n",
    "print(ft.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(-1))\n",
    "print(ft.unsqueeze(-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "lt = torch.LongTensor([1,2,3,4])\n",
    "print(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "print(lt.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0, 1], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "bt = torch.ByteTensor([True,True,False,True])\n",
    "print(bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0, 1])\n",
      "tensor([1., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(bt.long())\n",
    "print(bt.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1,2],[3,4]])\n",
    "y = torch.FloatTensor([[5,6],[7,8]])\n",
    "print(torch.cat([x,y], dim=0))\n",
    "print(torch.cat([x,y], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([1,4])\n",
    "y = torch.FloatTensor([2,5])\n",
    "z = torch.FloatTensor([3,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack([x,y,z]))\n",
    "print(torch.stack([x,y,z], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onez and Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [2., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[0,1,2], [2,1,0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones_like(x))\n",
    "print(torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1,2], [3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.mul(2.))\n",
    "print(x)\n",
    "print(x.mul_(2.))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = H(x) = Wx + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$cost(W,b) = {1 \\over m} \\sum_{i=1}^{m} (H(x^{(i)}) - y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 | W : 0.000 | Cost : 18.666666\n",
      "Epoch    1/1000 | W : 0.187 | Cost : 14.770963\n",
      "Epoch    2/1000 | W : 0.353 | Cost : 11.691541\n",
      "Epoch    3/1000 | W : 0.500 | Cost : 9.257346\n",
      "Epoch    4/1000 | W : 0.632 | Cost : 7.333169\n",
      "Epoch    5/1000 | W : 0.749 | Cost : 5.812135\n",
      "Epoch    6/1000 | W : 0.853 | Cost : 4.609764\n",
      "Epoch    7/1000 | W : 0.945 | Cost : 3.659278\n",
      "Epoch    8/1000 | W : 1.028 | Cost : 2.907896\n",
      "Epoch    9/1000 | W : 1.101 | Cost : 2.313895\n",
      "Epoch   10/1000 | W : 1.166 | Cost : 1.844294\n",
      "Epoch   11/1000 | W : 1.224 | Cost : 1.473027\n",
      "Epoch   12/1000 | W : 1.276 | Cost : 1.179487\n",
      "Epoch   13/1000 | W : 1.322 | Cost : 0.947386\n",
      "Epoch   14/1000 | W : 1.363 | Cost : 0.763851\n",
      "Epoch   15/1000 | W : 1.400 | Cost : 0.618704\n",
      "Epoch   16/1000 | W : 1.433 | Cost : 0.503902\n",
      "Epoch   17/1000 | W : 1.462 | Cost : 0.413086\n",
      "Epoch   18/1000 | W : 1.488 | Cost : 0.341229\n",
      "Epoch   19/1000 | W : 1.511 | Cost : 0.284360\n",
      "Epoch   20/1000 | W : 1.531 | Cost : 0.239337\n",
      "Epoch   21/1000 | W : 1.550 | Cost : 0.203679\n",
      "Epoch   22/1000 | W : 1.566 | Cost : 0.175424\n",
      "Epoch   23/1000 | W : 1.581 | Cost : 0.153021\n",
      "Epoch   24/1000 | W : 1.594 | Cost : 0.135243\n",
      "Epoch   25/1000 | W : 1.606 | Cost : 0.121122\n",
      "Epoch   26/1000 | W : 1.617 | Cost : 0.109892\n",
      "Epoch   27/1000 | W : 1.626 | Cost : 0.100947\n",
      "Epoch   28/1000 | W : 1.635 | Cost : 0.093809\n",
      "Epoch   29/1000 | W : 1.642 | Cost : 0.088100\n",
      "Epoch   30/1000 | W : 1.649 | Cost : 0.083519\n",
      "Epoch   31/1000 | W : 1.655 | Cost : 0.079833\n",
      "Epoch   32/1000 | W : 1.661 | Cost : 0.076852\n",
      "Epoch   33/1000 | W : 1.666 | Cost : 0.074430\n",
      "Epoch   34/1000 | W : 1.670 | Cost : 0.072450\n",
      "Epoch   35/1000 | W : 1.675 | Cost : 0.070819\n",
      "Epoch   36/1000 | W : 1.678 | Cost : 0.069465\n",
      "Epoch   37/1000 | W : 1.682 | Cost : 0.068330\n",
      "Epoch   38/1000 | W : 1.685 | Cost : 0.067368\n",
      "Epoch   39/1000 | W : 1.688 | Cost : 0.066544\n",
      "Epoch   40/1000 | W : 1.690 | Cost : 0.065829\n",
      "Epoch   41/1000 | W : 1.693 | Cost : 0.065200\n",
      "Epoch   42/1000 | W : 1.695 | Cost : 0.064639\n",
      "Epoch   43/1000 | W : 1.697 | Cost : 0.064133\n",
      "Epoch   44/1000 | W : 1.699 | Cost : 0.063671\n",
      "Epoch   45/1000 | W : 1.701 | Cost : 0.063243\n",
      "Epoch   46/1000 | W : 1.702 | Cost : 0.062843\n",
      "Epoch   47/1000 | W : 1.704 | Cost : 0.062465\n",
      "Epoch   48/1000 | W : 1.705 | Cost : 0.062104\n",
      "Epoch   49/1000 | W : 1.707 | Cost : 0.061758\n",
      "Epoch   50/1000 | W : 1.708 | Cost : 0.061424\n",
      "Epoch   51/1000 | W : 1.709 | Cost : 0.061099\n",
      "Epoch   52/1000 | W : 1.710 | Cost : 0.060782\n",
      "Epoch   53/1000 | W : 1.711 | Cost : 0.060472\n",
      "Epoch   54/1000 | W : 1.712 | Cost : 0.060167\n",
      "Epoch   55/1000 | W : 1.713 | Cost : 0.059866\n",
      "Epoch   56/1000 | W : 1.714 | Cost : 0.059570\n",
      "Epoch   57/1000 | W : 1.715 | Cost : 0.059276\n",
      "Epoch   58/1000 | W : 1.716 | Cost : 0.058986\n",
      "Epoch   59/1000 | W : 1.717 | Cost : 0.058698\n",
      "Epoch   60/1000 | W : 1.718 | Cost : 0.058413\n",
      "Epoch   61/1000 | W : 1.719 | Cost : 0.058129\n",
      "Epoch   62/1000 | W : 1.720 | Cost : 0.057848\n",
      "Epoch   63/1000 | W : 1.720 | Cost : 0.057568\n",
      "Epoch   64/1000 | W : 1.721 | Cost : 0.057290\n",
      "Epoch   65/1000 | W : 1.722 | Cost : 0.057014\n",
      "Epoch   66/1000 | W : 1.723 | Cost : 0.056740\n",
      "Epoch   67/1000 | W : 1.723 | Cost : 0.056466\n",
      "Epoch   68/1000 | W : 1.724 | Cost : 0.056195\n",
      "Epoch   69/1000 | W : 1.725 | Cost : 0.055924\n",
      "Epoch   70/1000 | W : 1.726 | Cost : 0.055656\n",
      "Epoch   71/1000 | W : 1.726 | Cost : 0.055388\n",
      "Epoch   72/1000 | W : 1.727 | Cost : 0.055122\n",
      "Epoch   73/1000 | W : 1.728 | Cost : 0.054857\n",
      "Epoch   74/1000 | W : 1.728 | Cost : 0.054593\n",
      "Epoch   75/1000 | W : 1.729 | Cost : 0.054331\n",
      "Epoch   76/1000 | W : 1.730 | Cost : 0.054070\n",
      "Epoch   77/1000 | W : 1.730 | Cost : 0.053810\n",
      "Epoch   78/1000 | W : 1.731 | Cost : 0.053552\n",
      "Epoch   79/1000 | W : 1.732 | Cost : 0.053295\n",
      "Epoch   80/1000 | W : 1.732 | Cost : 0.053039\n",
      "Epoch   81/1000 | W : 1.733 | Cost : 0.052784\n",
      "Epoch   82/1000 | W : 1.734 | Cost : 0.052531\n",
      "Epoch   83/1000 | W : 1.734 | Cost : 0.052278\n",
      "Epoch   84/1000 | W : 1.735 | Cost : 0.052027\n",
      "Epoch   85/1000 | W : 1.736 | Cost : 0.051777\n",
      "Epoch   86/1000 | W : 1.736 | Cost : 0.051529\n",
      "Epoch   87/1000 | W : 1.737 | Cost : 0.051281\n",
      "Epoch   88/1000 | W : 1.738 | Cost : 0.051035\n",
      "Epoch   89/1000 | W : 1.738 | Cost : 0.050790\n",
      "Epoch   90/1000 | W : 1.739 | Cost : 0.050546\n",
      "Epoch   91/1000 | W : 1.739 | Cost : 0.050303\n",
      "Epoch   92/1000 | W : 1.740 | Cost : 0.050062\n",
      "Epoch   93/1000 | W : 1.741 | Cost : 0.049821\n",
      "Epoch   94/1000 | W : 1.741 | Cost : 0.049582\n",
      "Epoch   95/1000 | W : 1.742 | Cost : 0.049344\n",
      "Epoch   96/1000 | W : 1.743 | Cost : 0.049107\n",
      "Epoch   97/1000 | W : 1.743 | Cost : 0.048871\n",
      "Epoch   98/1000 | W : 1.744 | Cost : 0.048637\n",
      "Epoch   99/1000 | W : 1.744 | Cost : 0.048403\n",
      "Epoch  100/1000 | W : 1.745 | Cost : 0.048171\n",
      "Epoch  101/1000 | W : 1.746 | Cost : 0.047939\n",
      "Epoch  102/1000 | W : 1.746 | Cost : 0.047709\n",
      "Epoch  103/1000 | W : 1.747 | Cost : 0.047480\n",
      "Epoch  104/1000 | W : 1.748 | Cost : 0.047252\n",
      "Epoch  105/1000 | W : 1.748 | Cost : 0.047025\n",
      "Epoch  106/1000 | W : 1.749 | Cost : 0.046799\n",
      "Epoch  107/1000 | W : 1.749 | Cost : 0.046574\n",
      "Epoch  108/1000 | W : 1.750 | Cost : 0.046351\n",
      "Epoch  109/1000 | W : 1.751 | Cost : 0.046128\n",
      "Epoch  110/1000 | W : 1.751 | Cost : 0.045907\n",
      "Epoch  111/1000 | W : 1.752 | Cost : 0.045686\n",
      "Epoch  112/1000 | W : 1.752 | Cost : 0.045467\n",
      "Epoch  113/1000 | W : 1.753 | Cost : 0.045249\n",
      "Epoch  114/1000 | W : 1.754 | Cost : 0.045031\n",
      "Epoch  115/1000 | W : 1.754 | Cost : 0.044815\n",
      "Epoch  116/1000 | W : 1.755 | Cost : 0.044600\n",
      "Epoch  117/1000 | W : 1.755 | Cost : 0.044386\n",
      "Epoch  118/1000 | W : 1.756 | Cost : 0.044172\n",
      "Epoch  119/1000 | W : 1.756 | Cost : 0.043960\n",
      "Epoch  120/1000 | W : 1.757 | Cost : 0.043749\n",
      "Epoch  121/1000 | W : 1.758 | Cost : 0.043539\n",
      "Epoch  122/1000 | W : 1.758 | Cost : 0.043330\n",
      "Epoch  123/1000 | W : 1.759 | Cost : 0.043122\n",
      "Epoch  124/1000 | W : 1.759 | Cost : 0.042915\n",
      "Epoch  125/1000 | W : 1.760 | Cost : 0.042709\n",
      "Epoch  126/1000 | W : 1.761 | Cost : 0.042504\n",
      "Epoch  127/1000 | W : 1.761 | Cost : 0.042300\n",
      "Epoch  128/1000 | W : 1.762 | Cost : 0.042097\n",
      "Epoch  129/1000 | W : 1.762 | Cost : 0.041894\n",
      "Epoch  130/1000 | W : 1.763 | Cost : 0.041693\n",
      "Epoch  131/1000 | W : 1.763 | Cost : 0.041493\n",
      "Epoch  132/1000 | W : 1.764 | Cost : 0.041294\n",
      "Epoch  133/1000 | W : 1.765 | Cost : 0.041095\n",
      "Epoch  134/1000 | W : 1.765 | Cost : 0.040898\n",
      "Epoch  135/1000 | W : 1.766 | Cost : 0.040702\n",
      "Epoch  136/1000 | W : 1.766 | Cost : 0.040506\n",
      "Epoch  137/1000 | W : 1.767 | Cost : 0.040312\n",
      "Epoch  138/1000 | W : 1.767 | Cost : 0.040118\n",
      "Epoch  139/1000 | W : 1.768 | Cost : 0.039926\n",
      "Epoch  140/1000 | W : 1.768 | Cost : 0.039734\n",
      "Epoch  141/1000 | W : 1.769 | Cost : 0.039543\n",
      "Epoch  142/1000 | W : 1.770 | Cost : 0.039353\n",
      "Epoch  143/1000 | W : 1.770 | Cost : 0.039164\n",
      "Epoch  144/1000 | W : 1.771 | Cost : 0.038976\n",
      "Epoch  145/1000 | W : 1.771 | Cost : 0.038789\n",
      "Epoch  146/1000 | W : 1.772 | Cost : 0.038603\n",
      "Epoch  147/1000 | W : 1.772 | Cost : 0.038417\n",
      "Epoch  148/1000 | W : 1.773 | Cost : 0.038233\n",
      "Epoch  149/1000 | W : 1.773 | Cost : 0.038049\n",
      "Epoch  150/1000 | W : 1.774 | Cost : 0.037866\n",
      "Epoch  151/1000 | W : 1.775 | Cost : 0.037685\n",
      "Epoch  152/1000 | W : 1.775 | Cost : 0.037504\n",
      "Epoch  153/1000 | W : 1.776 | Cost : 0.037324\n",
      "Epoch  154/1000 | W : 1.776 | Cost : 0.037144\n",
      "Epoch  155/1000 | W : 1.777 | Cost : 0.036966\n",
      "Epoch  156/1000 | W : 1.777 | Cost : 0.036788\n",
      "Epoch  157/1000 | W : 1.778 | Cost : 0.036612\n",
      "Epoch  158/1000 | W : 1.778 | Cost : 0.036436\n",
      "Epoch  159/1000 | W : 1.779 | Cost : 0.036261\n",
      "Epoch  160/1000 | W : 1.779 | Cost : 0.036087\n",
      "Epoch  161/1000 | W : 1.780 | Cost : 0.035914\n",
      "Epoch  162/1000 | W : 1.780 | Cost : 0.035741\n",
      "Epoch  163/1000 | W : 1.781 | Cost : 0.035569\n",
      "Epoch  164/1000 | W : 1.781 | Cost : 0.035399\n",
      "Epoch  165/1000 | W : 1.782 | Cost : 0.035229\n",
      "Epoch  166/1000 | W : 1.783 | Cost : 0.035060\n",
      "Epoch  167/1000 | W : 1.783 | Cost : 0.034891\n",
      "Epoch  168/1000 | W : 1.784 | Cost : 0.034724\n",
      "Epoch  169/1000 | W : 1.784 | Cost : 0.034557\n",
      "Epoch  170/1000 | W : 1.785 | Cost : 0.034391\n",
      "Epoch  171/1000 | W : 1.785 | Cost : 0.034226\n",
      "Epoch  172/1000 | W : 1.786 | Cost : 0.034061\n",
      "Epoch  173/1000 | W : 1.786 | Cost : 0.033898\n",
      "Epoch  174/1000 | W : 1.787 | Cost : 0.033735\n",
      "Epoch  175/1000 | W : 1.787 | Cost : 0.033573\n",
      "Epoch  176/1000 | W : 1.788 | Cost : 0.033412\n",
      "Epoch  177/1000 | W : 1.788 | Cost : 0.033251\n",
      "Epoch  178/1000 | W : 1.789 | Cost : 0.033092\n",
      "Epoch  179/1000 | W : 1.789 | Cost : 0.032933\n",
      "Epoch  180/1000 | W : 1.790 | Cost : 0.032775\n",
      "Epoch  181/1000 | W : 1.790 | Cost : 0.032617\n",
      "Epoch  182/1000 | W : 1.791 | Cost : 0.032461\n",
      "Epoch  183/1000 | W : 1.791 | Cost : 0.032305\n",
      "Epoch  184/1000 | W : 1.792 | Cost : 0.032150\n",
      "Epoch  185/1000 | W : 1.792 | Cost : 0.031995\n",
      "Epoch  186/1000 | W : 1.793 | Cost : 0.031842\n",
      "Epoch  187/1000 | W : 1.793 | Cost : 0.031689\n",
      "Epoch  188/1000 | W : 1.794 | Cost : 0.031537\n",
      "Epoch  189/1000 | W : 1.794 | Cost : 0.031385\n",
      "Epoch  190/1000 | W : 1.795 | Cost : 0.031234\n",
      "Epoch  191/1000 | W : 1.795 | Cost : 0.031084\n",
      "Epoch  192/1000 | W : 1.796 | Cost : 0.030935\n",
      "Epoch  193/1000 | W : 1.796 | Cost : 0.030787\n",
      "Epoch  194/1000 | W : 1.797 | Cost : 0.030639\n",
      "Epoch  195/1000 | W : 1.797 | Cost : 0.030492\n",
      "Epoch  196/1000 | W : 1.798 | Cost : 0.030345\n",
      "Epoch  197/1000 | W : 1.798 | Cost : 0.030199\n",
      "Epoch  198/1000 | W : 1.799 | Cost : 0.030054\n",
      "Epoch  199/1000 | W : 1.799 | Cost : 0.029910\n",
      "Epoch  200/1000 | W : 1.800 | Cost : 0.029767\n",
      "Epoch  201/1000 | W : 1.800 | Cost : 0.029624\n",
      "Epoch  202/1000 | W : 1.801 | Cost : 0.029481\n",
      "Epoch  203/1000 | W : 1.801 | Cost : 0.029340\n",
      "Epoch  204/1000 | W : 1.802 | Cost : 0.029199\n",
      "Epoch  205/1000 | W : 1.802 | Cost : 0.029059\n",
      "Epoch  206/1000 | W : 1.802 | Cost : 0.028919\n",
      "Epoch  207/1000 | W : 1.803 | Cost : 0.028780\n",
      "Epoch  208/1000 | W : 1.803 | Cost : 0.028642\n",
      "Epoch  209/1000 | W : 1.804 | Cost : 0.028504\n",
      "Epoch  210/1000 | W : 1.804 | Cost : 0.028368\n",
      "Epoch  211/1000 | W : 1.805 | Cost : 0.028231\n",
      "Epoch  212/1000 | W : 1.805 | Cost : 0.028096\n",
      "Epoch  213/1000 | W : 1.806 | Cost : 0.027961\n",
      "Epoch  214/1000 | W : 1.806 | Cost : 0.027827\n",
      "Epoch  215/1000 | W : 1.807 | Cost : 0.027693\n",
      "Epoch  216/1000 | W : 1.807 | Cost : 0.027560\n",
      "Epoch  217/1000 | W : 1.808 | Cost : 0.027428\n",
      "Epoch  218/1000 | W : 1.808 | Cost : 0.027296\n",
      "Epoch  219/1000 | W : 1.809 | Cost : 0.027165\n",
      "Epoch  220/1000 | W : 1.809 | Cost : 0.027034\n",
      "Epoch  221/1000 | W : 1.809 | Cost : 0.026905\n",
      "Epoch  222/1000 | W : 1.810 | Cost : 0.026775\n",
      "Epoch  223/1000 | W : 1.810 | Cost : 0.026647\n",
      "Epoch  224/1000 | W : 1.811 | Cost : 0.026519\n",
      "Epoch  225/1000 | W : 1.811 | Cost : 0.026392\n",
      "Epoch  226/1000 | W : 1.812 | Cost : 0.026265\n",
      "Epoch  227/1000 | W : 1.812 | Cost : 0.026139\n",
      "Epoch  228/1000 | W : 1.813 | Cost : 0.026013\n",
      "Epoch  229/1000 | W : 1.813 | Cost : 0.025888\n",
      "Epoch  230/1000 | W : 1.814 | Cost : 0.025764\n",
      "Epoch  231/1000 | W : 1.814 | Cost : 0.025640\n",
      "Epoch  232/1000 | W : 1.814 | Cost : 0.025517\n",
      "Epoch  233/1000 | W : 1.815 | Cost : 0.025395\n",
      "Epoch  234/1000 | W : 1.815 | Cost : 0.025273\n",
      "Epoch  235/1000 | W : 1.816 | Cost : 0.025151\n",
      "Epoch  236/1000 | W : 1.816 | Cost : 0.025030\n",
      "Epoch  237/1000 | W : 1.817 | Cost : 0.024910\n",
      "Epoch  238/1000 | W : 1.817 | Cost : 0.024791\n",
      "Epoch  239/1000 | W : 1.818 | Cost : 0.024672\n",
      "Epoch  240/1000 | W : 1.818 | Cost : 0.024553\n",
      "Epoch  241/1000 | W : 1.818 | Cost : 0.024435\n",
      "Epoch  242/1000 | W : 1.819 | Cost : 0.024318\n",
      "Epoch  243/1000 | W : 1.819 | Cost : 0.024201\n",
      "Epoch  244/1000 | W : 1.820 | Cost : 0.024085\n",
      "Epoch  245/1000 | W : 1.820 | Cost : 0.023969\n",
      "Epoch  246/1000 | W : 1.821 | Cost : 0.023854\n",
      "Epoch  247/1000 | W : 1.821 | Cost : 0.023740\n",
      "Epoch  248/1000 | W : 1.821 | Cost : 0.023626\n",
      "Epoch  249/1000 | W : 1.822 | Cost : 0.023512\n",
      "Epoch  250/1000 | W : 1.822 | Cost : 0.023399\n",
      "Epoch  251/1000 | W : 1.823 | Cost : 0.023287\n",
      "Epoch  252/1000 | W : 1.823 | Cost : 0.023175\n",
      "Epoch  253/1000 | W : 1.824 | Cost : 0.023064\n",
      "Epoch  254/1000 | W : 1.824 | Cost : 0.022953\n",
      "Epoch  255/1000 | W : 1.824 | Cost : 0.022843\n",
      "Epoch  256/1000 | W : 1.825 | Cost : 0.022733\n",
      "Epoch  257/1000 | W : 1.825 | Cost : 0.022624\n",
      "Epoch  258/1000 | W : 1.826 | Cost : 0.022515\n",
      "Epoch  259/1000 | W : 1.826 | Cost : 0.022407\n",
      "Epoch  260/1000 | W : 1.827 | Cost : 0.022299\n",
      "Epoch  261/1000 | W : 1.827 | Cost : 0.022192\n",
      "Epoch  262/1000 | W : 1.827 | Cost : 0.022086\n",
      "Epoch  263/1000 | W : 1.828 | Cost : 0.021980\n",
      "Epoch  264/1000 | W : 1.828 | Cost : 0.021874\n",
      "Epoch  265/1000 | W : 1.829 | Cost : 0.021769\n",
      "Epoch  266/1000 | W : 1.829 | Cost : 0.021665\n",
      "Epoch  267/1000 | W : 1.829 | Cost : 0.021561\n",
      "Epoch  268/1000 | W : 1.830 | Cost : 0.021457\n",
      "Epoch  269/1000 | W : 1.830 | Cost : 0.021354\n",
      "Epoch  270/1000 | W : 1.831 | Cost : 0.021252\n",
      "Epoch  271/1000 | W : 1.831 | Cost : 0.021149\n",
      "Epoch  272/1000 | W : 1.831 | Cost : 0.021048\n",
      "Epoch  273/1000 | W : 1.832 | Cost : 0.020947\n",
      "Epoch  274/1000 | W : 1.832 | Cost : 0.020846\n",
      "Epoch  275/1000 | W : 1.833 | Cost : 0.020746\n",
      "Epoch  276/1000 | W : 1.833 | Cost : 0.020646\n",
      "Epoch  277/1000 | W : 1.834 | Cost : 0.020547\n",
      "Epoch  278/1000 | W : 1.834 | Cost : 0.020449\n",
      "Epoch  279/1000 | W : 1.834 | Cost : 0.020351\n",
      "Epoch  280/1000 | W : 1.835 | Cost : 0.020253\n",
      "Epoch  281/1000 | W : 1.835 | Cost : 0.020156\n",
      "Epoch  282/1000 | W : 1.836 | Cost : 0.020059\n",
      "Epoch  283/1000 | W : 1.836 | Cost : 0.019962\n",
      "Epoch  284/1000 | W : 1.836 | Cost : 0.019867\n",
      "Epoch  285/1000 | W : 1.837 | Cost : 0.019771\n",
      "Epoch  286/1000 | W : 1.837 | Cost : 0.019676\n",
      "Epoch  287/1000 | W : 1.837 | Cost : 0.019582\n",
      "Epoch  288/1000 | W : 1.838 | Cost : 0.019488\n",
      "Epoch  289/1000 | W : 1.838 | Cost : 0.019394\n",
      "Epoch  290/1000 | W : 1.839 | Cost : 0.019301\n",
      "Epoch  291/1000 | W : 1.839 | Cost : 0.019208\n",
      "Epoch  292/1000 | W : 1.839 | Cost : 0.019116\n",
      "Epoch  293/1000 | W : 1.840 | Cost : 0.019024\n",
      "Epoch  294/1000 | W : 1.840 | Cost : 0.018933\n",
      "Epoch  295/1000 | W : 1.841 | Cost : 0.018842\n",
      "Epoch  296/1000 | W : 1.841 | Cost : 0.018751\n",
      "Epoch  297/1000 | W : 1.841 | Cost : 0.018661\n",
      "Epoch  298/1000 | W : 1.842 | Cost : 0.018572\n",
      "Epoch  299/1000 | W : 1.842 | Cost : 0.018483\n",
      "Epoch  300/1000 | W : 1.842 | Cost : 0.018394\n",
      "Epoch  301/1000 | W : 1.843 | Cost : 0.018306\n",
      "Epoch  302/1000 | W : 1.843 | Cost : 0.018218\n",
      "Epoch  303/1000 | W : 1.844 | Cost : 0.018130\n",
      "Epoch  304/1000 | W : 1.844 | Cost : 0.018043\n",
      "Epoch  305/1000 | W : 1.844 | Cost : 0.017956\n",
      "Epoch  306/1000 | W : 1.845 | Cost : 0.017870\n",
      "Epoch  307/1000 | W : 1.845 | Cost : 0.017784\n",
      "Epoch  308/1000 | W : 1.845 | Cost : 0.017699\n",
      "Epoch  309/1000 | W : 1.846 | Cost : 0.017614\n",
      "Epoch  310/1000 | W : 1.846 | Cost : 0.017529\n",
      "Epoch  311/1000 | W : 1.847 | Cost : 0.017445\n",
      "Epoch  312/1000 | W : 1.847 | Cost : 0.017361\n",
      "Epoch  313/1000 | W : 1.847 | Cost : 0.017278\n",
      "Epoch  314/1000 | W : 1.848 | Cost : 0.017195\n",
      "Epoch  315/1000 | W : 1.848 | Cost : 0.017113\n",
      "Epoch  316/1000 | W : 1.848 | Cost : 0.017030\n",
      "Epoch  317/1000 | W : 1.849 | Cost : 0.016949\n",
      "Epoch  318/1000 | W : 1.849 | Cost : 0.016867\n",
      "Epoch  319/1000 | W : 1.850 | Cost : 0.016786\n",
      "Epoch  320/1000 | W : 1.850 | Cost : 0.016706\n",
      "Epoch  321/1000 | W : 1.850 | Cost : 0.016625\n",
      "Epoch  322/1000 | W : 1.851 | Cost : 0.016546\n",
      "Epoch  323/1000 | W : 1.851 | Cost : 0.016466\n",
      "Epoch  324/1000 | W : 1.851 | Cost : 0.016387\n",
      "Epoch  325/1000 | W : 1.852 | Cost : 0.016308\n",
      "Epoch  326/1000 | W : 1.852 | Cost : 0.016230\n",
      "Epoch  327/1000 | W : 1.852 | Cost : 0.016152\n",
      "Epoch  328/1000 | W : 1.853 | Cost : 0.016075\n",
      "Epoch  329/1000 | W : 1.853 | Cost : 0.015997\n",
      "Epoch  330/1000 | W : 1.853 | Cost : 0.015921\n",
      "Epoch  331/1000 | W : 1.854 | Cost : 0.015844\n",
      "Epoch  332/1000 | W : 1.854 | Cost : 0.015768\n",
      "Epoch  333/1000 | W : 1.855 | Cost : 0.015692\n",
      "Epoch  334/1000 | W : 1.855 | Cost : 0.015617\n",
      "Epoch  335/1000 | W : 1.855 | Cost : 0.015542\n",
      "Epoch  336/1000 | W : 1.856 | Cost : 0.015467\n",
      "Epoch  337/1000 | W : 1.856 | Cost : 0.015393\n",
      "Epoch  338/1000 | W : 1.856 | Cost : 0.015319\n",
      "Epoch  339/1000 | W : 1.857 | Cost : 0.015246\n",
      "Epoch  340/1000 | W : 1.857 | Cost : 0.015172\n",
      "Epoch  341/1000 | W : 1.857 | Cost : 0.015099\n",
      "Epoch  342/1000 | W : 1.858 | Cost : 0.015027\n",
      "Epoch  343/1000 | W : 1.858 | Cost : 0.014955\n",
      "Epoch  344/1000 | W : 1.858 | Cost : 0.014883\n",
      "Epoch  345/1000 | W : 1.859 | Cost : 0.014812\n",
      "Epoch  346/1000 | W : 1.859 | Cost : 0.014740\n",
      "Epoch  347/1000 | W : 1.859 | Cost : 0.014670\n",
      "Epoch  348/1000 | W : 1.860 | Cost : 0.014599\n",
      "Epoch  349/1000 | W : 1.860 | Cost : 0.014529\n",
      "Epoch  350/1000 | W : 1.860 | Cost : 0.014459\n",
      "Epoch  351/1000 | W : 1.861 | Cost : 0.014390\n",
      "Epoch  352/1000 | W : 1.861 | Cost : 0.014321\n",
      "Epoch  353/1000 | W : 1.861 | Cost : 0.014252\n",
      "Epoch  354/1000 | W : 1.862 | Cost : 0.014184\n",
      "Epoch  355/1000 | W : 1.862 | Cost : 0.014115\n",
      "Epoch  356/1000 | W : 1.862 | Cost : 0.014048\n",
      "Epoch  357/1000 | W : 1.863 | Cost : 0.013980\n",
      "Epoch  358/1000 | W : 1.863 | Cost : 0.013913\n",
      "Epoch  359/1000 | W : 1.863 | Cost : 0.013846\n",
      "Epoch  360/1000 | W : 1.864 | Cost : 0.013780\n",
      "Epoch  361/1000 | W : 1.864 | Cost : 0.013714\n",
      "Epoch  362/1000 | W : 1.864 | Cost : 0.013648\n",
      "Epoch  363/1000 | W : 1.865 | Cost : 0.013582\n",
      "Epoch  364/1000 | W : 1.865 | Cost : 0.013517\n",
      "Epoch  365/1000 | W : 1.865 | Cost : 0.013452\n",
      "Epoch  366/1000 | W : 1.866 | Cost : 0.013387\n",
      "Epoch  367/1000 | W : 1.866 | Cost : 0.013323\n",
      "Epoch  368/1000 | W : 1.866 | Cost : 0.013259\n",
      "Epoch  369/1000 | W : 1.867 | Cost : 0.013196\n",
      "Epoch  370/1000 | W : 1.867 | Cost : 0.013132\n",
      "Epoch  371/1000 | W : 1.867 | Cost : 0.013069\n",
      "Epoch  372/1000 | W : 1.868 | Cost : 0.013006\n",
      "Epoch  373/1000 | W : 1.868 | Cost : 0.012944\n",
      "Epoch  374/1000 | W : 1.868 | Cost : 0.012882\n",
      "Epoch  375/1000 | W : 1.868 | Cost : 0.012820\n",
      "Epoch  376/1000 | W : 1.869 | Cost : 0.012758\n",
      "Epoch  377/1000 | W : 1.869 | Cost : 0.012697\n",
      "Epoch  378/1000 | W : 1.869 | Cost : 0.012636\n",
      "Epoch  379/1000 | W : 1.870 | Cost : 0.012575\n",
      "Epoch  380/1000 | W : 1.870 | Cost : 0.012515\n",
      "Epoch  381/1000 | W : 1.870 | Cost : 0.012455\n",
      "Epoch  382/1000 | W : 1.871 | Cost : 0.012395\n",
      "Epoch  383/1000 | W : 1.871 | Cost : 0.012336\n",
      "Epoch  384/1000 | W : 1.871 | Cost : 0.012276\n",
      "Epoch  385/1000 | W : 1.872 | Cost : 0.012217\n",
      "Epoch  386/1000 | W : 1.872 | Cost : 0.012159\n",
      "Epoch  387/1000 | W : 1.872 | Cost : 0.012100\n",
      "Epoch  388/1000 | W : 1.873 | Cost : 0.012042\n",
      "Epoch  389/1000 | W : 1.873 | Cost : 0.011984\n",
      "Epoch  390/1000 | W : 1.873 | Cost : 0.011927\n",
      "Epoch  391/1000 | W : 1.873 | Cost : 0.011870\n",
      "Epoch  392/1000 | W : 1.874 | Cost : 0.011813\n",
      "Epoch  393/1000 | W : 1.874 | Cost : 0.011756\n",
      "Epoch  394/1000 | W : 1.874 | Cost : 0.011699\n",
      "Epoch  395/1000 | W : 1.875 | Cost : 0.011643\n",
      "Epoch  396/1000 | W : 1.875 | Cost : 0.011587\n",
      "Epoch  397/1000 | W : 1.875 | Cost : 0.011532\n",
      "Epoch  398/1000 | W : 1.876 | Cost : 0.011476\n",
      "Epoch  399/1000 | W : 1.876 | Cost : 0.011421\n",
      "Epoch  400/1000 | W : 1.876 | Cost : 0.011366\n",
      "Epoch  401/1000 | W : 1.876 | Cost : 0.011312\n",
      "Epoch  402/1000 | W : 1.877 | Cost : 0.011257\n",
      "Epoch  403/1000 | W : 1.877 | Cost : 0.011203\n",
      "Epoch  404/1000 | W : 1.877 | Cost : 0.011150\n",
      "Epoch  405/1000 | W : 1.878 | Cost : 0.011096\n",
      "Epoch  406/1000 | W : 1.878 | Cost : 0.011043\n",
      "Epoch  407/1000 | W : 1.878 | Cost : 0.010990\n",
      "Epoch  408/1000 | W : 1.879 | Cost : 0.010937\n",
      "Epoch  409/1000 | W : 1.879 | Cost : 0.010884\n",
      "Epoch  410/1000 | W : 1.879 | Cost : 0.010832\n",
      "Epoch  411/1000 | W : 1.879 | Cost : 0.010780\n",
      "Epoch  412/1000 | W : 1.880 | Cost : 0.010728\n",
      "Epoch  413/1000 | W : 1.880 | Cost : 0.010677\n",
      "Epoch  414/1000 | W : 1.880 | Cost : 0.010626\n",
      "Epoch  415/1000 | W : 1.881 | Cost : 0.010575\n",
      "Epoch  416/1000 | W : 1.881 | Cost : 0.010524\n",
      "Epoch  417/1000 | W : 1.881 | Cost : 0.010473\n",
      "Epoch  418/1000 | W : 1.881 | Cost : 0.010423\n",
      "Epoch  419/1000 | W : 1.882 | Cost : 0.010373\n",
      "Epoch  420/1000 | W : 1.882 | Cost : 0.010323\n",
      "Epoch  421/1000 | W : 1.882 | Cost : 0.010274\n",
      "Epoch  422/1000 | W : 1.883 | Cost : 0.010224\n",
      "Epoch  423/1000 | W : 1.883 | Cost : 0.010175\n",
      "Epoch  424/1000 | W : 1.883 | Cost : 0.010126\n",
      "Epoch  425/1000 | W : 1.883 | Cost : 0.010078\n",
      "Epoch  426/1000 | W : 1.884 | Cost : 0.010029\n",
      "Epoch  427/1000 | W : 1.884 | Cost : 0.009981\n",
      "Epoch  428/1000 | W : 1.884 | Cost : 0.009933\n",
      "Epoch  429/1000 | W : 1.885 | Cost : 0.009885\n",
      "Epoch  430/1000 | W : 1.885 | Cost : 0.009838\n",
      "Epoch  431/1000 | W : 1.885 | Cost : 0.009791\n",
      "Epoch  432/1000 | W : 1.885 | Cost : 0.009744\n",
      "Epoch  433/1000 | W : 1.886 | Cost : 0.009697\n",
      "Epoch  434/1000 | W : 1.886 | Cost : 0.009650\n",
      "Epoch  435/1000 | W : 1.886 | Cost : 0.009604\n",
      "Epoch  436/1000 | W : 1.886 | Cost : 0.009558\n",
      "Epoch  437/1000 | W : 1.887 | Cost : 0.009512\n",
      "Epoch  438/1000 | W : 1.887 | Cost : 0.009466\n",
      "Epoch  439/1000 | W : 1.887 | Cost : 0.009421\n",
      "Epoch  440/1000 | W : 1.888 | Cost : 0.009376\n",
      "Epoch  441/1000 | W : 1.888 | Cost : 0.009331\n",
      "Epoch  442/1000 | W : 1.888 | Cost : 0.009286\n",
      "Epoch  443/1000 | W : 1.888 | Cost : 0.009241\n",
      "Epoch  444/1000 | W : 1.889 | Cost : 0.009197\n",
      "Epoch  445/1000 | W : 1.889 | Cost : 0.009153\n",
      "Epoch  446/1000 | W : 1.889 | Cost : 0.009109\n",
      "Epoch  447/1000 | W : 1.889 | Cost : 0.009065\n",
      "Epoch  448/1000 | W : 1.890 | Cost : 0.009021\n",
      "Epoch  449/1000 | W : 1.890 | Cost : 0.008978\n",
      "Epoch  450/1000 | W : 1.890 | Cost : 0.008935\n",
      "Epoch  451/1000 | W : 1.890 | Cost : 0.008892\n",
      "Epoch  452/1000 | W : 1.891 | Cost : 0.008849\n",
      "Epoch  453/1000 | W : 1.891 | Cost : 0.008807\n",
      "Epoch  454/1000 | W : 1.891 | Cost : 0.008765\n",
      "Epoch  455/1000 | W : 1.892 | Cost : 0.008722\n",
      "Epoch  456/1000 | W : 1.892 | Cost : 0.008681\n",
      "Epoch  457/1000 | W : 1.892 | Cost : 0.008639\n",
      "Epoch  458/1000 | W : 1.892 | Cost : 0.008597\n",
      "Epoch  459/1000 | W : 1.893 | Cost : 0.008556\n",
      "Epoch  460/1000 | W : 1.893 | Cost : 0.008515\n",
      "Epoch  461/1000 | W : 1.893 | Cost : 0.008474\n",
      "Epoch  462/1000 | W : 1.893 | Cost : 0.008433\n",
      "Epoch  463/1000 | W : 1.894 | Cost : 0.008393\n",
      "Epoch  464/1000 | W : 1.894 | Cost : 0.008353\n",
      "Epoch  465/1000 | W : 1.894 | Cost : 0.008313\n",
      "Epoch  466/1000 | W : 1.894 | Cost : 0.008273\n",
      "Epoch  467/1000 | W : 1.895 | Cost : 0.008233\n",
      "Epoch  468/1000 | W : 1.895 | Cost : 0.008193\n",
      "Epoch  469/1000 | W : 1.895 | Cost : 0.008154\n",
      "Epoch  470/1000 | W : 1.895 | Cost : 0.008115\n",
      "Epoch  471/1000 | W : 1.896 | Cost : 0.008076\n",
      "Epoch  472/1000 | W : 1.896 | Cost : 0.008037\n",
      "Epoch  473/1000 | W : 1.896 | Cost : 0.007999\n",
      "Epoch  474/1000 | W : 1.896 | Cost : 0.007960\n",
      "Epoch  475/1000 | W : 1.897 | Cost : 0.007922\n",
      "Epoch  476/1000 | W : 1.897 | Cost : 0.007884\n",
      "Epoch  477/1000 | W : 1.897 | Cost : 0.007846\n",
      "Epoch  478/1000 | W : 1.897 | Cost : 0.007808\n",
      "Epoch  479/1000 | W : 1.898 | Cost : 0.007771\n",
      "Epoch  480/1000 | W : 1.898 | Cost : 0.007733\n",
      "Epoch  481/1000 | W : 1.898 | Cost : 0.007696\n",
      "Epoch  482/1000 | W : 1.898 | Cost : 0.007659\n",
      "Epoch  483/1000 | W : 1.899 | Cost : 0.007623\n",
      "Epoch  484/1000 | W : 1.899 | Cost : 0.007586\n",
      "Epoch  485/1000 | W : 1.899 | Cost : 0.007550\n",
      "Epoch  486/1000 | W : 1.899 | Cost : 0.007513\n",
      "Epoch  487/1000 | W : 1.900 | Cost : 0.007477\n",
      "Epoch  488/1000 | W : 1.900 | Cost : 0.007441\n",
      "Epoch  489/1000 | W : 1.900 | Cost : 0.007406\n",
      "Epoch  490/1000 | W : 1.900 | Cost : 0.007370\n",
      "Epoch  491/1000 | W : 1.901 | Cost : 0.007335\n",
      "Epoch  492/1000 | W : 1.901 | Cost : 0.007299\n",
      "Epoch  493/1000 | W : 1.901 | Cost : 0.007264\n",
      "Epoch  494/1000 | W : 1.901 | Cost : 0.007229\n",
      "Epoch  495/1000 | W : 1.901 | Cost : 0.007195\n",
      "Epoch  496/1000 | W : 1.902 | Cost : 0.007160\n",
      "Epoch  497/1000 | W : 1.902 | Cost : 0.007126\n",
      "Epoch  498/1000 | W : 1.902 | Cost : 0.007092\n",
      "Epoch  499/1000 | W : 1.902 | Cost : 0.007058\n",
      "Epoch  500/1000 | W : 1.903 | Cost : 0.007024\n",
      "Epoch  501/1000 | W : 1.903 | Cost : 0.006990\n",
      "Epoch  502/1000 | W : 1.903 | Cost : 0.006956\n",
      "Epoch  503/1000 | W : 1.903 | Cost : 0.006923\n",
      "Epoch  504/1000 | W : 1.904 | Cost : 0.006890\n",
      "Epoch  505/1000 | W : 1.904 | Cost : 0.006857\n",
      "Epoch  506/1000 | W : 1.904 | Cost : 0.006824\n",
      "Epoch  507/1000 | W : 1.904 | Cost : 0.006791\n",
      "Epoch  508/1000 | W : 1.905 | Cost : 0.006758\n",
      "Epoch  509/1000 | W : 1.905 | Cost : 0.006726\n",
      "Epoch  510/1000 | W : 1.905 | Cost : 0.006694\n",
      "Epoch  511/1000 | W : 1.905 | Cost : 0.006661\n",
      "Epoch  512/1000 | W : 1.905 | Cost : 0.006629\n",
      "Epoch  513/1000 | W : 1.906 | Cost : 0.006598\n",
      "Epoch  514/1000 | W : 1.906 | Cost : 0.006566\n",
      "Epoch  515/1000 | W : 1.906 | Cost : 0.006534\n",
      "Epoch  516/1000 | W : 1.906 | Cost : 0.006503\n",
      "Epoch  517/1000 | W : 1.907 | Cost : 0.006472\n",
      "Epoch  518/1000 | W : 1.907 | Cost : 0.006441\n",
      "Epoch  519/1000 | W : 1.907 | Cost : 0.006410\n",
      "Epoch  520/1000 | W : 1.907 | Cost : 0.006379\n",
      "Epoch  521/1000 | W : 1.907 | Cost : 0.006348\n",
      "Epoch  522/1000 | W : 1.908 | Cost : 0.006318\n",
      "Epoch  523/1000 | W : 1.908 | Cost : 0.006288\n",
      "Epoch  524/1000 | W : 1.908 | Cost : 0.006257\n",
      "Epoch  525/1000 | W : 1.908 | Cost : 0.006227\n",
      "Epoch  526/1000 | W : 1.909 | Cost : 0.006197\n",
      "Epoch  527/1000 | W : 1.909 | Cost : 0.006168\n",
      "Epoch  528/1000 | W : 1.909 | Cost : 0.006138\n",
      "Epoch  529/1000 | W : 1.909 | Cost : 0.006109\n",
      "Epoch  530/1000 | W : 1.909 | Cost : 0.006079\n",
      "Epoch  531/1000 | W : 1.910 | Cost : 0.006050\n",
      "Epoch  532/1000 | W : 1.910 | Cost : 0.006021\n",
      "Epoch  533/1000 | W : 1.910 | Cost : 0.005992\n",
      "Epoch  534/1000 | W : 1.910 | Cost : 0.005963\n",
      "Epoch  535/1000 | W : 1.911 | Cost : 0.005935\n",
      "Epoch  536/1000 | W : 1.911 | Cost : 0.005906\n",
      "Epoch  537/1000 | W : 1.911 | Cost : 0.005878\n",
      "Epoch  538/1000 | W : 1.911 | Cost : 0.005850\n",
      "Epoch  539/1000 | W : 1.911 | Cost : 0.005821\n",
      "Epoch  540/1000 | W : 1.912 | Cost : 0.005794\n",
      "Epoch  541/1000 | W : 1.912 | Cost : 0.005766\n",
      "Epoch  542/1000 | W : 1.912 | Cost : 0.005738\n",
      "Epoch  543/1000 | W : 1.912 | Cost : 0.005710\n",
      "Epoch  544/1000 | W : 1.912 | Cost : 0.005683\n",
      "Epoch  545/1000 | W : 1.913 | Cost : 0.005656\n",
      "Epoch  546/1000 | W : 1.913 | Cost : 0.005629\n",
      "Epoch  547/1000 | W : 1.913 | Cost : 0.005602\n",
      "Epoch  548/1000 | W : 1.913 | Cost : 0.005575\n",
      "Epoch  549/1000 | W : 1.913 | Cost : 0.005548\n",
      "Epoch  550/1000 | W : 1.914 | Cost : 0.005521\n",
      "Epoch  551/1000 | W : 1.914 | Cost : 0.005495\n",
      "Epoch  552/1000 | W : 1.914 | Cost : 0.005468\n",
      "Epoch  553/1000 | W : 1.914 | Cost : 0.005442\n",
      "Epoch  554/1000 | W : 1.915 | Cost : 0.005416\n",
      "Epoch  555/1000 | W : 1.915 | Cost : 0.005390\n",
      "Epoch  556/1000 | W : 1.915 | Cost : 0.005364\n",
      "Epoch  557/1000 | W : 1.915 | Cost : 0.005338\n",
      "Epoch  558/1000 | W : 1.915 | Cost : 0.005313\n",
      "Epoch  559/1000 | W : 1.916 | Cost : 0.005287\n",
      "Epoch  560/1000 | W : 1.916 | Cost : 0.005262\n",
      "Epoch  561/1000 | W : 1.916 | Cost : 0.005237\n",
      "Epoch  562/1000 | W : 1.916 | Cost : 0.005211\n",
      "Epoch  563/1000 | W : 1.916 | Cost : 0.005186\n",
      "Epoch  564/1000 | W : 1.917 | Cost : 0.005161\n",
      "Epoch  565/1000 | W : 1.917 | Cost : 0.005137\n",
      "Epoch  566/1000 | W : 1.917 | Cost : 0.005112\n",
      "Epoch  567/1000 | W : 1.917 | Cost : 0.005087\n",
      "Epoch  568/1000 | W : 1.917 | Cost : 0.005063\n",
      "Epoch  569/1000 | W : 1.918 | Cost : 0.005039\n",
      "Epoch  570/1000 | W : 1.918 | Cost : 0.005014\n",
      "Epoch  571/1000 | W : 1.918 | Cost : 0.004990\n",
      "Epoch  572/1000 | W : 1.918 | Cost : 0.004966\n",
      "Epoch  573/1000 | W : 1.918 | Cost : 0.004943\n",
      "Epoch  574/1000 | W : 1.919 | Cost : 0.004919\n",
      "Epoch  575/1000 | W : 1.919 | Cost : 0.004895\n",
      "Epoch  576/1000 | W : 1.919 | Cost : 0.004872\n",
      "Epoch  577/1000 | W : 1.919 | Cost : 0.004848\n",
      "Epoch  578/1000 | W : 1.919 | Cost : 0.004825\n",
      "Epoch  579/1000 | W : 1.920 | Cost : 0.004802\n",
      "Epoch  580/1000 | W : 1.920 | Cost : 0.004779\n",
      "Epoch  581/1000 | W : 1.920 | Cost : 0.004756\n",
      "Epoch  582/1000 | W : 1.920 | Cost : 0.004733\n",
      "Epoch  583/1000 | W : 1.920 | Cost : 0.004710\n",
      "Epoch  584/1000 | W : 1.920 | Cost : 0.004688\n",
      "Epoch  585/1000 | W : 1.921 | Cost : 0.004665\n",
      "Epoch  586/1000 | W : 1.921 | Cost : 0.004643\n",
      "Epoch  587/1000 | W : 1.921 | Cost : 0.004620\n",
      "Epoch  588/1000 | W : 1.921 | Cost : 0.004598\n",
      "Epoch  589/1000 | W : 1.921 | Cost : 0.004576\n",
      "Epoch  590/1000 | W : 1.922 | Cost : 0.004554\n",
      "Epoch  591/1000 | W : 1.922 | Cost : 0.004532\n",
      "Epoch  592/1000 | W : 1.922 | Cost : 0.004511\n",
      "Epoch  593/1000 | W : 1.922 | Cost : 0.004489\n",
      "Epoch  594/1000 | W : 1.922 | Cost : 0.004467\n",
      "Epoch  595/1000 | W : 1.923 | Cost : 0.004446\n",
      "Epoch  596/1000 | W : 1.923 | Cost : 0.004425\n",
      "Epoch  597/1000 | W : 1.923 | Cost : 0.004403\n",
      "Epoch  598/1000 | W : 1.923 | Cost : 0.004382\n",
      "Epoch  599/1000 | W : 1.923 | Cost : 0.004361\n",
      "Epoch  600/1000 | W : 1.923 | Cost : 0.004340\n",
      "Epoch  601/1000 | W : 1.924 | Cost : 0.004319\n",
      "Epoch  602/1000 | W : 1.924 | Cost : 0.004299\n",
      "Epoch  603/1000 | W : 1.924 | Cost : 0.004278\n",
      "Epoch  604/1000 | W : 1.924 | Cost : 0.004257\n",
      "Epoch  605/1000 | W : 1.924 | Cost : 0.004237\n",
      "Epoch  606/1000 | W : 1.925 | Cost : 0.004217\n",
      "Epoch  607/1000 | W : 1.925 | Cost : 0.004196\n",
      "Epoch  608/1000 | W : 1.925 | Cost : 0.004176\n",
      "Epoch  609/1000 | W : 1.925 | Cost : 0.004156\n",
      "Epoch  610/1000 | W : 1.925 | Cost : 0.004136\n",
      "Epoch  611/1000 | W : 1.925 | Cost : 0.004116\n",
      "Epoch  612/1000 | W : 1.926 | Cost : 0.004097\n",
      "Epoch  613/1000 | W : 1.926 | Cost : 0.004077\n",
      "Epoch  614/1000 | W : 1.926 | Cost : 0.004057\n",
      "Epoch  615/1000 | W : 1.926 | Cost : 0.004038\n",
      "Epoch  616/1000 | W : 1.926 | Cost : 0.004018\n",
      "Epoch  617/1000 | W : 1.927 | Cost : 0.003999\n",
      "Epoch  618/1000 | W : 1.927 | Cost : 0.003980\n",
      "Epoch  619/1000 | W : 1.927 | Cost : 0.003961\n",
      "Epoch  620/1000 | W : 1.927 | Cost : 0.003942\n",
      "Epoch  621/1000 | W : 1.927 | Cost : 0.003923\n",
      "Epoch  622/1000 | W : 1.927 | Cost : 0.003904\n",
      "Epoch  623/1000 | W : 1.928 | Cost : 0.003885\n",
      "Epoch  624/1000 | W : 1.928 | Cost : 0.003867\n",
      "Epoch  625/1000 | W : 1.928 | Cost : 0.003848\n",
      "Epoch  626/1000 | W : 1.928 | Cost : 0.003830\n",
      "Epoch  627/1000 | W : 1.928 | Cost : 0.003811\n",
      "Epoch  628/1000 | W : 1.928 | Cost : 0.003793\n",
      "Epoch  629/1000 | W : 1.929 | Cost : 0.003775\n",
      "Epoch  630/1000 | W : 1.929 | Cost : 0.003757\n",
      "Epoch  631/1000 | W : 1.929 | Cost : 0.003739\n",
      "Epoch  632/1000 | W : 1.929 | Cost : 0.003721\n",
      "Epoch  633/1000 | W : 1.929 | Cost : 0.003703\n",
      "Epoch  634/1000 | W : 1.929 | Cost : 0.003685\n",
      "Epoch  635/1000 | W : 1.930 | Cost : 0.003667\n",
      "Epoch  636/1000 | W : 1.930 | Cost : 0.003650\n",
      "Epoch  637/1000 | W : 1.930 | Cost : 0.003632\n",
      "Epoch  638/1000 | W : 1.930 | Cost : 0.003615\n",
      "Epoch  639/1000 | W : 1.930 | Cost : 0.003597\n",
      "Epoch  640/1000 | W : 1.931 | Cost : 0.003580\n",
      "Epoch  641/1000 | W : 1.931 | Cost : 0.003563\n",
      "Epoch  642/1000 | W : 1.931 | Cost : 0.003546\n",
      "Epoch  643/1000 | W : 1.931 | Cost : 0.003529\n",
      "Epoch  644/1000 | W : 1.931 | Cost : 0.003512\n",
      "Epoch  645/1000 | W : 1.931 | Cost : 0.003495\n",
      "Epoch  646/1000 | W : 1.932 | Cost : 0.003478\n",
      "Epoch  647/1000 | W : 1.932 | Cost : 0.003461\n",
      "Epoch  648/1000 | W : 1.932 | Cost : 0.003445\n",
      "Epoch  649/1000 | W : 1.932 | Cost : 0.003428\n",
      "Epoch  650/1000 | W : 1.932 | Cost : 0.003412\n",
      "Epoch  651/1000 | W : 1.932 | Cost : 0.003395\n",
      "Epoch  652/1000 | W : 1.932 | Cost : 0.003379\n",
      "Epoch  653/1000 | W : 1.933 | Cost : 0.003363\n",
      "Epoch  654/1000 | W : 1.933 | Cost : 0.003347\n",
      "Epoch  655/1000 | W : 1.933 | Cost : 0.003331\n",
      "Epoch  656/1000 | W : 1.933 | Cost : 0.003315\n",
      "Epoch  657/1000 | W : 1.933 | Cost : 0.003299\n",
      "Epoch  658/1000 | W : 1.933 | Cost : 0.003283\n",
      "Epoch  659/1000 | W : 1.934 | Cost : 0.003267\n",
      "Epoch  660/1000 | W : 1.934 | Cost : 0.003251\n",
      "Epoch  661/1000 | W : 1.934 | Cost : 0.003236\n",
      "Epoch  662/1000 | W : 1.934 | Cost : 0.003220\n",
      "Epoch  663/1000 | W : 1.934 | Cost : 0.003205\n",
      "Epoch  664/1000 | W : 1.934 | Cost : 0.003189\n",
      "Epoch  665/1000 | W : 1.935 | Cost : 0.003174\n",
      "Epoch  666/1000 | W : 1.935 | Cost : 0.003159\n",
      "Epoch  667/1000 | W : 1.935 | Cost : 0.003144\n",
      "Epoch  668/1000 | W : 1.935 | Cost : 0.003129\n",
      "Epoch  669/1000 | W : 1.935 | Cost : 0.003114\n",
      "Epoch  670/1000 | W : 1.935 | Cost : 0.003099\n",
      "Epoch  671/1000 | W : 1.936 | Cost : 0.003084\n",
      "Epoch  672/1000 | W : 1.936 | Cost : 0.003069\n",
      "Epoch  673/1000 | W : 1.936 | Cost : 0.003054\n",
      "Epoch  674/1000 | W : 1.936 | Cost : 0.003040\n",
      "Epoch  675/1000 | W : 1.936 | Cost : 0.003025\n",
      "Epoch  676/1000 | W : 1.936 | Cost : 0.003010\n",
      "Epoch  677/1000 | W : 1.936 | Cost : 0.002996\n",
      "Epoch  678/1000 | W : 1.937 | Cost : 0.002982\n",
      "Epoch  679/1000 | W : 1.937 | Cost : 0.002967\n",
      "Epoch  680/1000 | W : 1.937 | Cost : 0.002953\n",
      "Epoch  681/1000 | W : 1.937 | Cost : 0.002939\n",
      "Epoch  682/1000 | W : 1.937 | Cost : 0.002925\n",
      "Epoch  683/1000 | W : 1.937 | Cost : 0.002911\n",
      "Epoch  684/1000 | W : 1.937 | Cost : 0.002897\n",
      "Epoch  685/1000 | W : 1.938 | Cost : 0.002883\n",
      "Epoch  686/1000 | W : 1.938 | Cost : 0.002869\n",
      "Epoch  687/1000 | W : 1.938 | Cost : 0.002855\n",
      "Epoch  688/1000 | W : 1.938 | Cost : 0.002841\n",
      "Epoch  689/1000 | W : 1.938 | Cost : 0.002828\n",
      "Epoch  690/1000 | W : 1.938 | Cost : 0.002814\n",
      "Epoch  691/1000 | W : 1.939 | Cost : 0.002801\n",
      "Epoch  692/1000 | W : 1.939 | Cost : 0.002787\n",
      "Epoch  693/1000 | W : 1.939 | Cost : 0.002774\n",
      "Epoch  694/1000 | W : 1.939 | Cost : 0.002761\n",
      "Epoch  695/1000 | W : 1.939 | Cost : 0.002747\n",
      "Epoch  696/1000 | W : 1.939 | Cost : 0.002734\n",
      "Epoch  697/1000 | W : 1.939 | Cost : 0.002721\n",
      "Epoch  698/1000 | W : 1.940 | Cost : 0.002708\n",
      "Epoch  699/1000 | W : 1.940 | Cost : 0.002695\n",
      "Epoch  700/1000 | W : 1.940 | Cost : 0.002682\n",
      "Epoch  701/1000 | W : 1.940 | Cost : 0.002669\n",
      "Epoch  702/1000 | W : 1.940 | Cost : 0.002656\n",
      "Epoch  703/1000 | W : 1.940 | Cost : 0.002644\n",
      "Epoch  704/1000 | W : 1.940 | Cost : 0.002631\n",
      "Epoch  705/1000 | W : 1.941 | Cost : 0.002618\n",
      "Epoch  706/1000 | W : 1.941 | Cost : 0.002606\n",
      "Epoch  707/1000 | W : 1.941 | Cost : 0.002593\n",
      "Epoch  708/1000 | W : 1.941 | Cost : 0.002581\n",
      "Epoch  709/1000 | W : 1.941 | Cost : 0.002568\n",
      "Epoch  710/1000 | W : 1.941 | Cost : 0.002556\n",
      "Epoch  711/1000 | W : 1.941 | Cost : 0.002544\n",
      "Epoch  712/1000 | W : 1.942 | Cost : 0.002531\n",
      "Epoch  713/1000 | W : 1.942 | Cost : 0.002519\n",
      "Epoch  714/1000 | W : 1.942 | Cost : 0.002507\n",
      "Epoch  715/1000 | W : 1.942 | Cost : 0.002495\n",
      "Epoch  716/1000 | W : 1.942 | Cost : 0.002483\n",
      "Epoch  717/1000 | W : 1.942 | Cost : 0.002471\n",
      "Epoch  718/1000 | W : 1.942 | Cost : 0.002459\n",
      "Epoch  719/1000 | W : 1.943 | Cost : 0.002448\n",
      "Epoch  720/1000 | W : 1.943 | Cost : 0.002436\n",
      "Epoch  721/1000 | W : 1.943 | Cost : 0.002424\n",
      "Epoch  722/1000 | W : 1.943 | Cost : 0.002412\n",
      "Epoch  723/1000 | W : 1.943 | Cost : 0.002401\n",
      "Epoch  724/1000 | W : 1.943 | Cost : 0.002389\n",
      "Epoch  725/1000 | W : 1.943 | Cost : 0.002378\n",
      "Epoch  726/1000 | W : 1.944 | Cost : 0.002366\n",
      "Epoch  727/1000 | W : 1.944 | Cost : 0.002355\n",
      "Epoch  728/1000 | W : 1.944 | Cost : 0.002344\n",
      "Epoch  729/1000 | W : 1.944 | Cost : 0.002333\n",
      "Epoch  730/1000 | W : 1.944 | Cost : 0.002321\n",
      "Epoch  731/1000 | W : 1.944 | Cost : 0.002310\n",
      "Epoch  732/1000 | W : 1.944 | Cost : 0.002299\n",
      "Epoch  733/1000 | W : 1.944 | Cost : 0.002288\n",
      "Epoch  734/1000 | W : 1.945 | Cost : 0.002277\n",
      "Epoch  735/1000 | W : 1.945 | Cost : 0.002266\n",
      "Epoch  736/1000 | W : 1.945 | Cost : 0.002255\n",
      "Epoch  737/1000 | W : 1.945 | Cost : 0.002244\n",
      "Epoch  738/1000 | W : 1.945 | Cost : 0.002234\n",
      "Epoch  739/1000 | W : 1.945 | Cost : 0.002223\n",
      "Epoch  740/1000 | W : 1.945 | Cost : 0.002212\n",
      "Epoch  741/1000 | W : 1.946 | Cost : 0.002202\n",
      "Epoch  742/1000 | W : 1.946 | Cost : 0.002191\n",
      "Epoch  743/1000 | W : 1.946 | Cost : 0.002181\n",
      "Epoch  744/1000 | W : 1.946 | Cost : 0.002170\n",
      "Epoch  745/1000 | W : 1.946 | Cost : 0.002160\n",
      "Epoch  746/1000 | W : 1.946 | Cost : 0.002149\n",
      "Epoch  747/1000 | W : 1.946 | Cost : 0.002139\n",
      "Epoch  748/1000 | W : 1.946 | Cost : 0.002129\n",
      "Epoch  749/1000 | W : 1.947 | Cost : 0.002118\n",
      "Epoch  750/1000 | W : 1.947 | Cost : 0.002108\n",
      "Epoch  751/1000 | W : 1.947 | Cost : 0.002098\n",
      "Epoch  752/1000 | W : 1.947 | Cost : 0.002088\n",
      "Epoch  753/1000 | W : 1.947 | Cost : 0.002078\n",
      "Epoch  754/1000 | W : 1.947 | Cost : 0.002068\n",
      "Epoch  755/1000 | W : 1.947 | Cost : 0.002058\n",
      "Epoch  756/1000 | W : 1.947 | Cost : 0.002048\n",
      "Epoch  757/1000 | W : 1.948 | Cost : 0.002038\n",
      "Epoch  758/1000 | W : 1.948 | Cost : 0.002029\n",
      "Epoch  759/1000 | W : 1.948 | Cost : 0.002019\n",
      "Epoch  760/1000 | W : 1.948 | Cost : 0.002009\n",
      "Epoch  761/1000 | W : 1.948 | Cost : 0.002000\n",
      "Epoch  762/1000 | W : 1.948 | Cost : 0.001990\n",
      "Epoch  763/1000 | W : 1.948 | Cost : 0.001980\n",
      "Epoch  764/1000 | W : 1.948 | Cost : 0.001971\n",
      "Epoch  765/1000 | W : 1.949 | Cost : 0.001961\n",
      "Epoch  766/1000 | W : 1.949 | Cost : 0.001952\n",
      "Epoch  767/1000 | W : 1.949 | Cost : 0.001943\n",
      "Epoch  768/1000 | W : 1.949 | Cost : 0.001933\n",
      "Epoch  769/1000 | W : 1.949 | Cost : 0.001924\n",
      "Epoch  770/1000 | W : 1.949 | Cost : 0.001915\n",
      "Epoch  771/1000 | W : 1.949 | Cost : 0.001906\n",
      "Epoch  772/1000 | W : 1.949 | Cost : 0.001896\n",
      "Epoch  773/1000 | W : 1.950 | Cost : 0.001887\n",
      "Epoch  774/1000 | W : 1.950 | Cost : 0.001878\n",
      "Epoch  775/1000 | W : 1.950 | Cost : 0.001869\n",
      "Epoch  776/1000 | W : 1.950 | Cost : 0.001860\n",
      "Epoch  777/1000 | W : 1.950 | Cost : 0.001851\n",
      "Epoch  778/1000 | W : 1.950 | Cost : 0.001842\n",
      "Epoch  779/1000 | W : 1.950 | Cost : 0.001834\n",
      "Epoch  780/1000 | W : 1.950 | Cost : 0.001825\n",
      "Epoch  781/1000 | W : 1.951 | Cost : 0.001816\n",
      "Epoch  782/1000 | W : 1.951 | Cost : 0.001807\n",
      "Epoch  783/1000 | W : 1.951 | Cost : 0.001799\n",
      "Epoch  784/1000 | W : 1.951 | Cost : 0.001790\n",
      "Epoch  785/1000 | W : 1.951 | Cost : 0.001781\n",
      "Epoch  786/1000 | W : 1.951 | Cost : 0.001773\n",
      "Epoch  787/1000 | W : 1.951 | Cost : 0.001764\n",
      "Epoch  788/1000 | W : 1.951 | Cost : 0.001756\n",
      "Epoch  789/1000 | W : 1.951 | Cost : 0.001747\n",
      "Epoch  790/1000 | W : 1.952 | Cost : 0.001739\n",
      "Epoch  791/1000 | W : 1.952 | Cost : 0.001731\n",
      "Epoch  792/1000 | W : 1.952 | Cost : 0.001722\n",
      "Epoch  793/1000 | W : 1.952 | Cost : 0.001714\n",
      "Epoch  794/1000 | W : 1.952 | Cost : 0.001706\n",
      "Epoch  795/1000 | W : 1.952 | Cost : 0.001698\n",
      "Epoch  796/1000 | W : 1.952 | Cost : 0.001690\n",
      "Epoch  797/1000 | W : 1.952 | Cost : 0.001681\n",
      "Epoch  798/1000 | W : 1.952 | Cost : 0.001673\n",
      "Epoch  799/1000 | W : 1.953 | Cost : 0.001665\n",
      "Epoch  800/1000 | W : 1.953 | Cost : 0.001657\n",
      "Epoch  801/1000 | W : 1.953 | Cost : 0.001649\n",
      "Epoch  802/1000 | W : 1.953 | Cost : 0.001641\n",
      "Epoch  803/1000 | W : 1.953 | Cost : 0.001634\n",
      "Epoch  804/1000 | W : 1.953 | Cost : 0.001626\n",
      "Epoch  805/1000 | W : 1.953 | Cost : 0.001618\n",
      "Epoch  806/1000 | W : 1.953 | Cost : 0.001610\n",
      "Epoch  807/1000 | W : 1.954 | Cost : 0.001602\n",
      "Epoch  808/1000 | W : 1.954 | Cost : 0.001595\n",
      "Epoch  809/1000 | W : 1.954 | Cost : 0.001587\n",
      "Epoch  810/1000 | W : 1.954 | Cost : 0.001579\n",
      "Epoch  811/1000 | W : 1.954 | Cost : 0.001572\n",
      "Epoch  812/1000 | W : 1.954 | Cost : 0.001564\n",
      "Epoch  813/1000 | W : 1.954 | Cost : 0.001557\n",
      "Epoch  814/1000 | W : 1.954 | Cost : 0.001549\n",
      "Epoch  815/1000 | W : 1.954 | Cost : 0.001542\n",
      "Epoch  816/1000 | W : 1.955 | Cost : 0.001534\n",
      "Epoch  817/1000 | W : 1.955 | Cost : 0.001527\n",
      "Epoch  818/1000 | W : 1.955 | Cost : 0.001520\n",
      "Epoch  819/1000 | W : 1.955 | Cost : 0.001512\n",
      "Epoch  820/1000 | W : 1.955 | Cost : 0.001505\n",
      "Epoch  821/1000 | W : 1.955 | Cost : 0.001498\n",
      "Epoch  822/1000 | W : 1.955 | Cost : 0.001491\n",
      "Epoch  823/1000 | W : 1.955 | Cost : 0.001484\n",
      "Epoch  824/1000 | W : 1.955 | Cost : 0.001476\n",
      "Epoch  825/1000 | W : 1.955 | Cost : 0.001469\n",
      "Epoch  826/1000 | W : 1.956 | Cost : 0.001462\n",
      "Epoch  827/1000 | W : 1.956 | Cost : 0.001455\n",
      "Epoch  828/1000 | W : 1.956 | Cost : 0.001448\n",
      "Epoch  829/1000 | W : 1.956 | Cost : 0.001441\n",
      "Epoch  830/1000 | W : 1.956 | Cost : 0.001434\n",
      "Epoch  831/1000 | W : 1.956 | Cost : 0.001428\n",
      "Epoch  832/1000 | W : 1.956 | Cost : 0.001421\n",
      "Epoch  833/1000 | W : 1.956 | Cost : 0.001414\n",
      "Epoch  834/1000 | W : 1.956 | Cost : 0.001407\n",
      "Epoch  835/1000 | W : 1.957 | Cost : 0.001400\n",
      "Epoch  836/1000 | W : 1.957 | Cost : 0.001394\n",
      "Epoch  837/1000 | W : 1.957 | Cost : 0.001387\n",
      "Epoch  838/1000 | W : 1.957 | Cost : 0.001380\n",
      "Epoch  839/1000 | W : 1.957 | Cost : 0.001374\n",
      "Epoch  840/1000 | W : 1.957 | Cost : 0.001367\n",
      "Epoch  841/1000 | W : 1.957 | Cost : 0.001360\n",
      "Epoch  842/1000 | W : 1.957 | Cost : 0.001354\n",
      "Epoch  843/1000 | W : 1.957 | Cost : 0.001347\n",
      "Epoch  844/1000 | W : 1.957 | Cost : 0.001341\n",
      "Epoch  845/1000 | W : 1.958 | Cost : 0.001335\n",
      "Epoch  846/1000 | W : 1.958 | Cost : 0.001328\n",
      "Epoch  847/1000 | W : 1.958 | Cost : 0.001322\n",
      "Epoch  848/1000 | W : 1.958 | Cost : 0.001315\n",
      "Epoch  849/1000 | W : 1.958 | Cost : 0.001309\n",
      "Epoch  850/1000 | W : 1.958 | Cost : 0.001303\n",
      "Epoch  851/1000 | W : 1.958 | Cost : 0.001297\n",
      "Epoch  852/1000 | W : 1.958 | Cost : 0.001290\n",
      "Epoch  853/1000 | W : 1.958 | Cost : 0.001284\n",
      "Epoch  854/1000 | W : 1.958 | Cost : 0.001278\n",
      "Epoch  855/1000 | W : 1.959 | Cost : 0.001272\n",
      "Epoch  856/1000 | W : 1.959 | Cost : 0.001266\n",
      "Epoch  857/1000 | W : 1.959 | Cost : 0.001260\n",
      "Epoch  858/1000 | W : 1.959 | Cost : 0.001254\n",
      "Epoch  859/1000 | W : 1.959 | Cost : 0.001248\n",
      "Epoch  860/1000 | W : 1.959 | Cost : 0.001242\n",
      "Epoch  861/1000 | W : 1.959 | Cost : 0.001236\n",
      "Epoch  862/1000 | W : 1.959 | Cost : 0.001230\n",
      "Epoch  863/1000 | W : 1.959 | Cost : 0.001224\n",
      "Epoch  864/1000 | W : 1.959 | Cost : 0.001218\n",
      "Epoch  865/1000 | W : 1.960 | Cost : 0.001212\n",
      "Epoch  866/1000 | W : 1.960 | Cost : 0.001206\n",
      "Epoch  867/1000 | W : 1.960 | Cost : 0.001200\n",
      "Epoch  868/1000 | W : 1.960 | Cost : 0.001195\n",
      "Epoch  869/1000 | W : 1.960 | Cost : 0.001189\n",
      "Epoch  870/1000 | W : 1.960 | Cost : 0.001183\n",
      "Epoch  871/1000 | W : 1.960 | Cost : 0.001178\n",
      "Epoch  872/1000 | W : 1.960 | Cost : 0.001172\n",
      "Epoch  873/1000 | W : 1.960 | Cost : 0.001166\n",
      "Epoch  874/1000 | W : 1.960 | Cost : 0.001161\n",
      "Epoch  875/1000 | W : 1.961 | Cost : 0.001155\n",
      "Epoch  876/1000 | W : 1.961 | Cost : 0.001150\n",
      "Epoch  877/1000 | W : 1.961 | Cost : 0.001144\n",
      "Epoch  878/1000 | W : 1.961 | Cost : 0.001139\n",
      "Epoch  879/1000 | W : 1.961 | Cost : 0.001133\n",
      "Epoch  880/1000 | W : 1.961 | Cost : 0.001128\n",
      "Epoch  881/1000 | W : 1.961 | Cost : 0.001122\n",
      "Epoch  882/1000 | W : 1.961 | Cost : 0.001117\n",
      "Epoch  883/1000 | W : 1.961 | Cost : 0.001111\n",
      "Epoch  884/1000 | W : 1.961 | Cost : 0.001106\n",
      "Epoch  885/1000 | W : 1.961 | Cost : 0.001101\n",
      "Epoch  886/1000 | W : 1.962 | Cost : 0.001096\n",
      "Epoch  887/1000 | W : 1.962 | Cost : 0.001090\n",
      "Epoch  888/1000 | W : 1.962 | Cost : 0.001085\n",
      "Epoch  889/1000 | W : 1.962 | Cost : 0.001080\n",
      "Epoch  890/1000 | W : 1.962 | Cost : 0.001075\n",
      "Epoch  891/1000 | W : 1.962 | Cost : 0.001069\n",
      "Epoch  892/1000 | W : 1.962 | Cost : 0.001064\n",
      "Epoch  893/1000 | W : 1.962 | Cost : 0.001059\n",
      "Epoch  894/1000 | W : 1.962 | Cost : 0.001054\n",
      "Epoch  895/1000 | W : 1.962 | Cost : 0.001049\n",
      "Epoch  896/1000 | W : 1.962 | Cost : 0.001044\n",
      "Epoch  897/1000 | W : 1.963 | Cost : 0.001039\n",
      "Epoch  898/1000 | W : 1.963 | Cost : 0.001034\n",
      "Epoch  899/1000 | W : 1.963 | Cost : 0.001029\n",
      "Epoch  900/1000 | W : 1.963 | Cost : 0.001024\n",
      "Epoch  901/1000 | W : 1.963 | Cost : 0.001019\n",
      "Epoch  902/1000 | W : 1.963 | Cost : 0.001014\n",
      "Epoch  903/1000 | W : 1.963 | Cost : 0.001009\n",
      "Epoch  904/1000 | W : 1.963 | Cost : 0.001005\n",
      "Epoch  905/1000 | W : 1.963 | Cost : 0.001000\n",
      "Epoch  906/1000 | W : 1.963 | Cost : 0.000995\n",
      "Epoch  907/1000 | W : 1.963 | Cost : 0.000990\n",
      "Epoch  908/1000 | W : 1.964 | Cost : 0.000985\n",
      "Epoch  909/1000 | W : 1.964 | Cost : 0.000981\n",
      "Epoch  910/1000 | W : 1.964 | Cost : 0.000976\n",
      "Epoch  911/1000 | W : 1.964 | Cost : 0.000971\n",
      "Epoch  912/1000 | W : 1.964 | Cost : 0.000967\n",
      "Epoch  913/1000 | W : 1.964 | Cost : 0.000962\n",
      "Epoch  914/1000 | W : 1.964 | Cost : 0.000957\n",
      "Epoch  915/1000 | W : 1.964 | Cost : 0.000953\n",
      "Epoch  916/1000 | W : 1.964 | Cost : 0.000948\n",
      "Epoch  917/1000 | W : 1.964 | Cost : 0.000944\n",
      "Epoch  918/1000 | W : 1.964 | Cost : 0.000939\n",
      "Epoch  919/1000 | W : 1.964 | Cost : 0.000935\n",
      "Epoch  920/1000 | W : 1.965 | Cost : 0.000930\n",
      "Epoch  921/1000 | W : 1.965 | Cost : 0.000926\n",
      "Epoch  922/1000 | W : 1.965 | Cost : 0.000921\n",
      "Epoch  923/1000 | W : 1.965 | Cost : 0.000917\n",
      "Epoch  924/1000 | W : 1.965 | Cost : 0.000912\n",
      "Epoch  925/1000 | W : 1.965 | Cost : 0.000908\n",
      "Epoch  926/1000 | W : 1.965 | Cost : 0.000904\n",
      "Epoch  927/1000 | W : 1.965 | Cost : 0.000899\n",
      "Epoch  928/1000 | W : 1.965 | Cost : 0.000895\n",
      "Epoch  929/1000 | W : 1.965 | Cost : 0.000891\n",
      "Epoch  930/1000 | W : 1.965 | Cost : 0.000886\n",
      "Epoch  931/1000 | W : 1.966 | Cost : 0.000882\n",
      "Epoch  932/1000 | W : 1.966 | Cost : 0.000878\n",
      "Epoch  933/1000 | W : 1.966 | Cost : 0.000874\n",
      "Epoch  934/1000 | W : 1.966 | Cost : 0.000870\n",
      "Epoch  935/1000 | W : 1.966 | Cost : 0.000865\n",
      "Epoch  936/1000 | W : 1.966 | Cost : 0.000861\n",
      "Epoch  937/1000 | W : 1.966 | Cost : 0.000857\n",
      "Epoch  938/1000 | W : 1.966 | Cost : 0.000853\n",
      "Epoch  939/1000 | W : 1.966 | Cost : 0.000849\n",
      "Epoch  940/1000 | W : 1.966 | Cost : 0.000845\n",
      "Epoch  941/1000 | W : 1.966 | Cost : 0.000841\n",
      "Epoch  942/1000 | W : 1.966 | Cost : 0.000837\n",
      "Epoch  943/1000 | W : 1.966 | Cost : 0.000833\n",
      "Epoch  944/1000 | W : 1.967 | Cost : 0.000829\n",
      "Epoch  945/1000 | W : 1.967 | Cost : 0.000825\n",
      "Epoch  946/1000 | W : 1.967 | Cost : 0.000821\n",
      "Epoch  947/1000 | W : 1.967 | Cost : 0.000817\n",
      "Epoch  948/1000 | W : 1.967 | Cost : 0.000813\n",
      "Epoch  949/1000 | W : 1.967 | Cost : 0.000809\n",
      "Epoch  950/1000 | W : 1.967 | Cost : 0.000805\n",
      "Epoch  951/1000 | W : 1.967 | Cost : 0.000801\n",
      "Epoch  952/1000 | W : 1.967 | Cost : 0.000797\n",
      "Epoch  953/1000 | W : 1.967 | Cost : 0.000794\n",
      "Epoch  954/1000 | W : 1.967 | Cost : 0.000790\n",
      "Epoch  955/1000 | W : 1.967 | Cost : 0.000786\n",
      "Epoch  956/1000 | W : 1.968 | Cost : 0.000782\n",
      "Epoch  957/1000 | W : 1.968 | Cost : 0.000778\n",
      "Epoch  958/1000 | W : 1.968 | Cost : 0.000775\n",
      "Epoch  959/1000 | W : 1.968 | Cost : 0.000771\n",
      "Epoch  960/1000 | W : 1.968 | Cost : 0.000767\n",
      "Epoch  961/1000 | W : 1.968 | Cost : 0.000764\n",
      "Epoch  962/1000 | W : 1.968 | Cost : 0.000760\n",
      "Epoch  963/1000 | W : 1.968 | Cost : 0.000756\n",
      "Epoch  964/1000 | W : 1.968 | Cost : 0.000753\n",
      "Epoch  965/1000 | W : 1.968 | Cost : 0.000749\n",
      "Epoch  966/1000 | W : 1.968 | Cost : 0.000745\n",
      "Epoch  967/1000 | W : 1.968 | Cost : 0.000742\n",
      "Epoch  968/1000 | W : 1.968 | Cost : 0.000738\n",
      "Epoch  969/1000 | W : 1.969 | Cost : 0.000735\n",
      "Epoch  970/1000 | W : 1.969 | Cost : 0.000731\n",
      "Epoch  971/1000 | W : 1.969 | Cost : 0.000728\n",
      "Epoch  972/1000 | W : 1.969 | Cost : 0.000724\n",
      "Epoch  973/1000 | W : 1.969 | Cost : 0.000721\n",
      "Epoch  974/1000 | W : 1.969 | Cost : 0.000717\n",
      "Epoch  975/1000 | W : 1.969 | Cost : 0.000714\n",
      "Epoch  976/1000 | W : 1.969 | Cost : 0.000710\n",
      "Epoch  977/1000 | W : 1.969 | Cost : 0.000707\n",
      "Epoch  978/1000 | W : 1.969 | Cost : 0.000704\n",
      "Epoch  979/1000 | W : 1.969 | Cost : 0.000700\n",
      "Epoch  980/1000 | W : 1.969 | Cost : 0.000697\n",
      "Epoch  981/1000 | W : 1.969 | Cost : 0.000693\n",
      "Epoch  982/1000 | W : 1.969 | Cost : 0.000690\n",
      "Epoch  983/1000 | W : 1.970 | Cost : 0.000687\n",
      "Epoch  984/1000 | W : 1.970 | Cost : 0.000684\n",
      "Epoch  985/1000 | W : 1.970 | Cost : 0.000680\n",
      "Epoch  986/1000 | W : 1.970 | Cost : 0.000677\n",
      "Epoch  987/1000 | W : 1.970 | Cost : 0.000674\n",
      "Epoch  988/1000 | W : 1.970 | Cost : 0.000670\n",
      "Epoch  989/1000 | W : 1.970 | Cost : 0.000667\n",
      "Epoch  990/1000 | W : 1.970 | Cost : 0.000664\n",
      "Epoch  991/1000 | W : 1.970 | Cost : 0.000661\n",
      "Epoch  992/1000 | W : 1.970 | Cost : 0.000658\n",
      "Epoch  993/1000 | W : 1.970 | Cost : 0.000655\n",
      "Epoch  994/1000 | W : 1.970 | Cost : 0.000651\n",
      "Epoch  995/1000 | W : 1.970 | Cost : 0.000648\n",
      "Epoch  996/1000 | W : 1.970 | Cost : 0.000645\n",
      "Epoch  997/1000 | W : 1.971 | Cost : 0.000642\n",
      "Epoch  998/1000 | W : 1.971 | Cost : 0.000639\n",
      "Epoch  999/1000 | W : 1.971 | Cost : 0.000636\n",
      "Epoch 1000/1000 | W : 1.971 | Cost : 0.000633\n"
     ]
    }
   ],
   "source": [
    "# Hypothesis\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "# Weight Bias 0 \n",
    "# requires_grad = True :   \n",
    "W  = torch.zeros(1, requires_grad=True)\n",
    "b  = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Epochs\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in range(nb_epochs+1) : \n",
    "    \n",
    "    # H(x) \n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # Cost \n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    print('Epoch {:4d}/{} | W : {:.3f} | Cost : {:.6f}' .format(epoch, nb_epochs, W.item(), cost.item()))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD([W,b], lr=0.01)\n",
    "    optimizer.zero_grad()                       # zero_grad() gradient \n",
    "    cost.backward()                             # backward() gradient \n",
    "    optimizer.step()                            # step "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Variable Linear Ragression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20  |  hypothesis: tensor([0., 0., 0., 0., 0.])  |  Cost: 29661.800781\n",
      "Epoch    1/20  |  hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605])  |  Cost: 9298.520508\n",
      "Epoch    2/20  |  hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821])  |  Cost: 2915.712402\n",
      "Epoch    3/20  |  hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097])  |  Cost: 915.040527\n",
      "Epoch    4/20  |  hypothesis: tensor([137.7967, 165.6247, 163.1911, 177.7112, 126.3307])  |  Cost: 287.936096\n",
      "Epoch    5/20  |  hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891])  |  Cost: 91.371071\n",
      "Epoch    6/20  |  hypothesis: tensor([148.1035, 178.0143, 175.3980, 191.0042, 135.7812])  |  Cost: 29.758249\n",
      "Epoch    7/20  |  hypothesis: tensor([150.1744, 180.5042, 177.8509, 193.6753, 137.6805])  |  Cost: 10.445267\n",
      "Epoch    8/20  |  hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440])  |  Cost: 4.391237\n",
      "Epoch    9/20  |  hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396])  |  Cost: 2.493121\n",
      "Epoch   10/20  |  hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732])  |  Cost: 1.897688\n",
      "Epoch   11/20  |  hypothesis: tensor([152.5485, 183.3609, 180.6640, 196.7389, 139.8602])  |  Cost: 1.710552\n",
      "Epoch   12/20  |  hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651])  |  Cost: 1.651416\n",
      "Epoch   13/20  |  hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240])  |  Cost: 1.632369\n",
      "Epoch   14/20  |  hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571])  |  Cost: 1.625924\n",
      "Epoch   15/20  |  hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759])  |  Cost: 1.623420\n",
      "Epoch   16/20  |  hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865])  |  Cost: 1.622152\n",
      "Epoch   17/20  |  hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927])  |  Cost: 1.621261\n",
      "Epoch   18/20  |  hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0661, 140.0963])  |  Cost: 1.620501\n",
      "Epoch   19/20  |  hypothesis: tensor([152.8014, 183.6715, 180.9665, 197.0686, 140.0985])  |  Cost: 1.619757\n",
      "Epoch   20/20  |  hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999])  |  Cost: 1.619046\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    " [93, 88, 93],\n",
    " [89, 91, 90],\n",
    " [96, 98, 100],\n",
    " [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "#  \n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer \n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # H(x) \n",
    "    hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "    # cost \n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    # cost H(x) \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch {:4d}/{}  |  hypothesis: {}  |  Cost: {:.6f}'\\\n",
    "          .format(epoch, nb_epochs, hypothesis.squeeze().detach(),cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.519531\n",
      "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712158\n",
      "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527\n",
      "Epoch    4/20 hypothesis: tensor([137.7967, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936096\n",
      "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371071\n",
      "Epoch    6/20 hypothesis: tensor([148.1035, 178.0143, 175.3980, 191.0042, 135.7812]) Cost: 29.758249\n",
      "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8509, 193.6753, 137.6805]) Cost: 10.445267\n",
      "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391237\n",
      "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493121\n",
      "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
      "Epoch   11/20 hypothesis: tensor([152.5485, 183.3609, 180.6640, 196.7389, 139.8602]) Cost: 1.710551\n",
      "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651416\n",
      "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632369\n",
      "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625924\n",
      "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623420\n",
      "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622152\n",
      "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621261\n",
      "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0661, 140.0963]) Cost: 1.620501\n",
      "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9665, 197.0686, 140.0985]) Cost: 1.619758\n",
      "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) Cost: 1.619046\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# cost \n",
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "# cost \n",
    "import torch.nn.functional as F\n",
    "cost = F.mse_loss(hypothesis, y_train)\n",
    "\n",
    "# \n",
    "x_train = torch.FloatTensor([[73, 80, 75],[93, 88, 93],[89, 91, 90],[96, 98, 100],[73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "#  \n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "# optimizer \n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) \n",
    "    hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "    model = MultivariateLinearRegressionModel()\n",
    "\n",
    "    Hypothesis = model(x_train)\n",
    "    \n",
    "    # cost \n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    cost = F.mse_loss(hypothesis, y_train)\n",
    "    \n",
    "    # cost H(x) \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'\\\n",
    "          .format(epoch, nb_epochs, hypothesis.squeeze().detach(),cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x_data = [[73, 80, 75],[93, 88, 93],[89, 91, 90],[96, 98, 100],[73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y\n",
    "    \n",
    "dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 21774.083984\n",
      "Epoch    0/20 Batch 2/3 Cost: 23790.875000\n",
      "Epoch    0/20 Batch 3/3 Cost: 32154.193359\n",
      "Epoch    1/20 Batch 1/3 Cost: 24475.066406\n",
      "Epoch    1/20 Batch 2/3 Cost: 22802.066406\n",
      "Epoch    1/20 Batch 3/3 Cost: 28729.841797\n",
      "Epoch    2/20 Batch 1/3 Cost: 27741.035156\n",
      "Epoch    2/20 Batch 2/3 Cost: 17823.923828\n",
      "Epoch    2/20 Batch 3/3 Cost: 32154.193359\n",
      "Epoch    3/20 Batch 1/3 Cost: 22762.890625\n",
      "Epoch    3/20 Batch 2/3 Cost: 25503.050781\n",
      "Epoch    3/20 Batch 3/3 Cost: 26752.226562\n",
      "Epoch    4/20 Batch 1/3 Cost: 21774.083984\n",
      "Epoch    4/20 Batch 2/3 Cost: 23790.875000\n",
      "Epoch    4/20 Batch 3/3 Cost: 32154.193359\n",
      "Epoch    5/20 Batch 1/3 Cost: 22802.066406\n",
      "Epoch    5/20 Batch 2/3 Cost: 30442.017578\n",
      "Epoch    5/20 Batch 3/3 Cost: 16795.939453\n",
      "Epoch    6/20 Batch 1/3 Cost: 29453.210938\n",
      "Epoch    6/20 Batch 2/3 Cost: 22762.890625\n",
      "Epoch    6/20 Batch 3/3 Cost: 18851.906250\n",
      "Epoch    7/20 Batch 1/3 Cost: 27741.035156\n",
      "Epoch    7/20 Batch 2/3 Cost: 25503.050781\n",
      "Epoch    7/20 Batch 3/3 Cost: 16795.939453\n",
      "Epoch    8/20 Batch 1/3 Cost: 24475.066406\n",
      "Epoch    8/20 Batch 2/3 Cost: 22802.066406\n",
      "Epoch    8/20 Batch 3/3 Cost: 28729.841797\n",
      "Epoch    9/20 Batch 1/3 Cost: 22802.066406\n",
      "Epoch    9/20 Batch 2/3 Cost: 30442.017578\n",
      "Epoch    9/20 Batch 3/3 Cost: 16795.939453\n",
      "Epoch   10/20 Batch 1/3 Cost: 30442.017578\n",
      "Epoch   10/20 Batch 2/3 Cost: 17823.923828\n",
      "Epoch   10/20 Batch 3/3 Cost: 26752.226562\n",
      "Epoch   11/20 Batch 1/3 Cost: 27741.035156\n",
      "Epoch   11/20 Batch 2/3 Cost: 24475.066406\n",
      "Epoch   11/20 Batch 3/3 Cost: 18851.906250\n",
      "Epoch   12/20 Batch 1/3 Cost: 21774.083984\n",
      "Epoch   12/20 Batch 2/3 Cost: 23790.875000\n",
      "Epoch   12/20 Batch 3/3 Cost: 32154.193359\n",
      "Epoch   13/20 Batch 1/3 Cost: 25503.050781\n",
      "Epoch   13/20 Batch 2/3 Cost: 21774.083984\n",
      "Epoch   13/20 Batch 3/3 Cost: 28729.841797\n",
      "Epoch   14/20 Batch 1/3 Cost: 22762.890625\n",
      "Epoch   14/20 Batch 2/3 Cost: 29453.210938\n",
      "Epoch   14/20 Batch 3/3 Cost: 18851.906250\n",
      "Epoch   15/20 Batch 1/3 Cost: 17823.923828\n",
      "Epoch   15/20 Batch 2/3 Cost: 30442.017578\n",
      "Epoch   15/20 Batch 3/3 Cost: 26752.226562\n",
      "Epoch   16/20 Batch 1/3 Cost: 22802.066406\n",
      "Epoch   16/20 Batch 2/3 Cost: 24475.066406\n",
      "Epoch   16/20 Batch 3/3 Cost: 28729.841797\n",
      "Epoch   17/20 Batch 1/3 Cost: 27741.035156\n",
      "Epoch   17/20 Batch 2/3 Cost: 24475.066406\n",
      "Epoch   17/20 Batch 3/3 Cost: 18851.906250\n",
      "Epoch   18/20 Batch 1/3 Cost: 27741.035156\n",
      "Epoch   18/20 Batch 2/3 Cost: 17823.923828\n",
      "Epoch   18/20 Batch 3/3 Cost: 32154.193359\n",
      "Epoch   19/20 Batch 1/3 Cost: 24475.066406\n",
      "Epoch   19/20 Batch 2/3 Cost: 23790.875000\n",
      "Epoch   19/20 Batch 3/3 Cost: 26752.226562\n",
      "Epoch   20/20 Batch 1/3 Cost: 22802.066406\n",
      "Epoch   20/20 Batch 2/3 Cost: 22762.890625\n",
      "Epoch   20/20 Batch 3/3 Cost: 32154.193359\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        \n",
    "        x_train, y_train = samples\n",
    "        \n",
    "        # H(x) \n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        # cost \n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        # cost H(x) \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'\\\n",
    "              .format(epoch, nb_epochs, batch_idx+1, len(dataloader),cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n",
      "\n",
      "e = tensor([2.7183])\n",
      "\n",
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<MulBackward0>)\n",
      "torch.Size([6, 1])\n",
      "\n",
      "1/e^{-1} = tensor([0.7311])\n",
      "\n",
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape, end='\\n\\n')\n",
    "\n",
    "print('e =', torch.exp(torch.FloatTensor([1])), end='\\n\\n')\n",
    "\n",
    "w = torch.zeros((2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(w) + b)))\n",
    "print(hypothesis)\n",
    "print(hypothesis.shape, end='\\n\\n')\n",
    "\n",
    "print('1/e^{-1} =', torch.sigmoid(torch.FloatTensor([1])), end='\\n\\n')\n",
    "\n",
    "hypothesis = torch.sigmoid((x_train.matmul(w) + b))\n",
    "print(hypothesis)\n",
    "print(hypothesis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "\n",
      "tensor([0.6931], grad_fn=<NegBackward>)\n",
      "\n",
      "tensor([[0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931]], grad_fn=<NegBackward>)\n",
      "\n",
      "tensor(0.6931, grad_fn=<MeanBackward0>)\n",
      "\n",
      "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(y_train, end='\\n\\n')\n",
    "\n",
    "print(-(y_train[0] * torch.log(hypothesis[0]) + (1 - y_train[0]) * torch.log(1 - hypothesis[0])), end='\\n\\n')\n",
    "\n",
    "losses = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis))\n",
    "print(losses, end='\\n\\n')\n",
    "\n",
    "cost = losses.mean()\n",
    "print(cost, end='\\n\\n')\n",
    "\n",
    "print(F.binary_cross_entropy(hypothesis, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    0/1000  |  cost = 0.693147\n",
      "epoch =  100/1000  |  cost = 0.578086\n",
      "epoch =  200/1000  |  cost = 0.535207\n",
      "epoch =  300/1000  |  cost = 0.505647\n",
      "epoch =  400/1000  |  cost = 0.484024\n",
      "epoch =  500/1000  |  cost = 0.467288\n",
      "epoch =  600/1000  |  cost = 0.453674\n",
      "epoch =  700/1000  |  cost = 0.442127\n",
      "epoch =  800/1000  |  cost = 0.431998\n",
      "epoch =  900/1000  |  cost = 0.422874\n",
      "epoch = 1000/1000  |  cost = 0.414490\n",
      "\n",
      "tensor([[0.2169],\n",
      "        [0.2644],\n",
      "        [0.6868],\n",
      "        [0.6295],\n",
      "        [0.7870]], grad_fn=<SliceBackward>)\n",
      "\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "\n",
      "tensor([[ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True]])\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W,b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in range(nb_epochs + 1) : \n",
    "    \n",
    "    # Cost \n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    # cost H(x) \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # epoch 100  log \n",
    "    if epoch % 100 == 0 : \n",
    "        print('epoch = {:4d}/{}  |  cost = {:.6f}' .format(epoch, nb_epochs, cost.item()))\n",
    "\n",
    "print()\n",
    "\n",
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "print(hypothesis[:5], end='\\n\\n')\n",
    "\n",
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction[:5].float())\n",
    "print(y_train[:5], end='\\n\\n')\n",
    "\n",
    "correct_prediction = prediction.float() == y_train\n",
    "print(correct_prediction[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [6 x 2], m2: [8 x 1] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:197",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-d1caf3be26a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-d1caf3be26a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [6 x 2], m2: [8 x 1] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:197"
     ]
    }
   ],
   "source": [
    "class BinaryClassifier(nn.Module) :\n",
    "    \n",
    "    def __init__(self) : \n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(8,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x) : \n",
    "        return self.sigmoid(self.linear(x))\n",
    "    \n",
    "model = BinaryClassifier()\n",
    "\n",
    "# Opitmizer \n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "# Epoch\n",
    "nb_epochs = 100\n",
    "\n",
    "for epoch in range(nb_epochs + 1) : \n",
    "    \n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    # Cost H(x) \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 20  \n",
    "    if epoch % 20 == 0 : \n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        print('epoch = {:4d}/{}  |  cost = {:.6f}  |  Accuracy = {:2.2f}%'\\\n",
    "              .format(epoch, nb_epochs, cost.item(), accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0ca98d4730>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.FloatTensor([1,2,3])\n",
    "\n",
    "hypothesis = F.softmax(z, dim=0)\n",
    "hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
       "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
       "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross Entropy Loss\n",
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "hypothesis = F.softmax(z, dim=1)\n",
    "hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.randint(5, (3,)).long()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter(1, y.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low Level\n",
    "torch.log(F.softmax(z, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High Level\n",
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low Level\n",
    "(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High Level\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1,2,1,1],\n",
    "           [2,1,3,2],\n",
    "           [3,1,3,4],\n",
    "           [4,1,5,5],\n",
    "           [1,7,5,5],\n",
    "           [1,2,5,6],\n",
    "           [1,6,6,6],\n",
    "           [1,7,7,7]]\n",
    "y_train = [2,2,2,1,1,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.FloatTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor [8, 3] and index [3, 1] to have the same size in dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-ee8052bea6a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0my_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0my_one_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_one_hot\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor [8, 3] and index [3, 1] to have the same size in dimension 1"
     ]
    }
   ],
   "source": [
    "#  \n",
    "W = torch.zeros((4,3), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "# Optimizer \n",
    "optimizer = optim.SGD([W,b], lr=0.01)\n",
    "\n",
    "# Epoch\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in range(nb_epochs +1) : \n",
    "    \n",
    "    # Cost  (1)\n",
    "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1)\n",
    "    y_one_hot = torch.zeros_like(hypothesis)\n",
    "    y_one_hot.scatter_(1, y.unsqueeze(1), 1)\n",
    "    cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "    \n",
    "    # Cost H(x) \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # epoch 100  log \n",
    "    if epoch % 100 == 0 : \n",
    "        print('epoch = {:4d}/{}  |  cost = {:.6f}' .format(epoch, nb_epochs, cost.item()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-3b923d548a3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Cost  (2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Cost H(x) \u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward"
     ]
    }
   ],
   "source": [
    "#  \n",
    "W = torch.zeros((4,3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Optimizer \n",
    "optimizer = optim.SGD([W,b], lr=0.1)\n",
    "\n",
    "# Epoch\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in range(nb_epochs +1) : \n",
    "    \n",
    "    # Cost  (2)\n",
    "    z = x_train.matmul(W) + b\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "    \n",
    "    # Cost H(x) \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # epoch 100  log \n",
    "    if epoch % 100 == 0 : \n",
    "        print('epoch = {:4d}/{}  |  cost = {:.6f}' .format(epoch, nb_epochs, cost.item()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module) : \n",
    "    def __init__(self) : \n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3)\n",
    "        \n",
    "    def forward(self, x) : \n",
    "        return self.linear(x)\n",
    "    \n",
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-e41384bd714d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Cost \u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Cost H(x) \u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward"
     ]
    }
   ],
   "source": [
    "# Optimizer \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Epochs\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in range(nb_epochs + 1) : \n",
    "    \n",
    "    # H(x) \n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # Cost \n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "    \n",
    "    # Cost H(x) \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # epoch 100  log \n",
    "    if epoch % 100 == 0 : \n",
    "        print('epoch = {:4d}/{}  |  cost = {:.6f}' .format(epoch, nb_epochs, cost.item()))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
